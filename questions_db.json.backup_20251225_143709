{
  "metadata": {
    "created": "2025-12-22T20:11:06.046252",
    "last_updated": "2025-12-25T11:27:39.857669",
    "version": "1.0"
  },
  "categories": {
    "Machine Learning": [
      {
        "question": "You're building a credit risk model and notice that your training accuracy is 98% but test accuracy is 72%. What is the most likely issue?",
        "options": [
          "The model is underfitting the data",
          "The model is overfitting the training data",
          "The test dataset is too small",
          "The features are not normalized"
        ],
        "correct_answer": 1,
        "explanation": "Overfitting occurs when a model learns the training data too well, including noise and outliers, resulting in poor generalization to new data. The large gap between training (98%) and test (72%) accuracy is a classic sign of overfitting. Common solutions include regularization, reducing model complexity, or increasing training data.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In a binary classification problem with 95% negative samples and 5% positive samples, which metric would be MOST misleading if used alone?",
        "options": [
          "F1-score",
          "Precision",
          "Accuracy",
          "ROC-AUC"
        ],
        "correct_answer": 2,
        "explanation": "Accuracy can be highly misleading in imbalanced datasets. A model that always predicts the majority class (negative) would achieve 95% accuracy without learning anything useful. F1-score, precision, recall, and ROC-AUC are better suited for imbalanced problems as they consider both classes' performance.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "You apply L2 regularization to your linear regression model. What effect does increasing the lambda (λ) parameter have?",
        "options": [
          "Increases model complexity and overfitting",
          "Decreases bias and increases variance",
          "Pushes feature weights closer to zero",
          "Automatically performs feature selection by setting weights to exactly zero"
        ],
        "correct_answer": 2,
        "explanation": "L2 regularization (Ridge) adds a penalty term proportional to the square of weights. Increasing λ penalizes large weights more heavily, pushing them closer to (but not exactly) zero. This reduces model complexity and helps prevent overfitting. L1 regularization (Lasso) is what sets weights to exactly zero, performing feature selection.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In k-fold cross-validation with k=5, what percentage of data is used for training in each fold?",
        "options": [
          "20%",
          "50%",
          "80%",
          "100%"
        ],
        "correct_answer": 2,
        "explanation": "In 5-fold cross-validation, the data is split into 5 equal parts. In each iteration, 4 parts (80%) are used for training and 1 part (20%) for validation. This process repeats 5 times, with each fold serving as the validation set once.",
        "difficulty": "Medium",
        "time_estimate": 70
      },
      {
        "question": "A data scientist notices their Random Forest model performs worse than a single Decision Tree. What is the most likely cause?",
        "options": [
          "Random Forests always perform worse than Decision Trees",
          "The trees in the forest are too correlated due to similar features being selected",
          "The number of trees is too high",
          "Random Forests can't handle categorical variables"
        ],
        "correct_answer": 1,
        "explanation": "Random Forests work by averaging predictions from multiple decorrelated trees. If the trees are highly correlated (e.g., due to limited feature diversity, too few features sampled per split, or highly imbalanced data), the ensemble loses its advantage. The strength of Random Forest comes from the diversity of its constituent trees.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In the context of supervised learning, which statement about the bias-variance tradeoff is correct?",
        "options": [
          "High bias and high variance both lead to overfitting",
          "Increasing model complexity always reduces both bias and variance",
          "Low bias and high variance typically indicate overfitting",
          "Regularization primarily reduces bias"
        ],
        "correct_answer": 2,
        "explanation": "Low bias means the model fits the training data well, while high variance means predictions vary significantly with different training sets. This combination is characteristic of overfitting - the model captures noise in training data but doesn't generalize well. Regularization typically reduces variance, not bias.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "You're using Gradient Boosting and notice training is very slow. Which hyperparameter would speed up training MOST without significantly hurting performance?",
        "options": [
          "Increase the learning rate significantly",
          "Reduce the maximum depth of trees",
          "Increase the number of estimators",
          "Remove all regularization"
        ],
        "correct_answer": 1,
        "explanation": "Reducing max depth makes trees shallower and faster to build. Gradient Boosting builds trees sequentially, so faster individual trees significantly speed up training. While increasing learning rate can reduce iterations needed, setting it too high can hurt convergence. Increasing estimators or removing regularization would slow training or hurt performance.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In a multi-class classification problem with 10 classes, what is the output shape of a one-hot encoded label vector for a single sample?",
        "options": [
          "(1,)",
          "(10,)",
          "(1, 10)",
          "(10, 1)"
        ],
        "correct_answer": 1,
        "explanation": "One-hot encoding creates a binary vector with length equal to the number of classes. For 10 classes, the vector has shape (10,) with all zeros except a 1 at the index corresponding to the class. For example, class 3 would be [0,0,0,1,0,0,0,0,0,0].",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "You're using K-Nearest Neighbors (KNN) for classification. The features have very different scales (e.g., age: 0-100, income: 0-1000000). What preprocessing step is MOST important?",
        "options": [
          "One-hot encoding",
          "Feature normalization/standardization",
          "PCA dimensionality reduction",
          "No preprocessing needed"
        ],
        "correct_answer": 1,
        "explanation": "KNN uses distance metrics to find nearest neighbors. Without normalization, features with larger scales (like income) will dominate the distance calculation, making smaller-scale features (like age) nearly irrelevant. Normalization (e.g., StandardScaler, MinMaxScaler) ensures all features contribute equally to distance calculations.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In the context of decision trees, what does 'pruning' accomplish?",
        "options": [
          "Removes features that are not important",
          "Reduces overfitting by removing branches that provide little predictive power",
          "Increases the maximum depth of the tree",
          "Balances the dataset by removing samples"
        ],
        "correct_answer": 1,
        "explanation": "Pruning removes sections of the tree that provide little power to classify instances, reducing complexity and preventing overfitting. Pre-pruning stops tree growth early using criteria like max depth, while post-pruning removes branches after the tree is fully grown. This is different from feature selection.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "You're building a model to predict housing prices. Which algorithm would be LEAST appropriate for this regression task?",
        "options": [
          "Linear Regression",
          "Random Forest Regressor",
          "Logistic Regression",
          "Gradient Boosting Regressor"
        ],
        "correct_answer": 2,
        "explanation": "Logistic Regression is specifically designed for classification tasks (predicting categories), not regression (predicting continuous values). Despite its name, it outputs probabilities for class membership. For housing price prediction, you need regression algorithms like Linear Regression, Random Forest Regressor, or Gradient Boosting Regressor.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "In ensemble learning, what is the primary difference between bagging and boosting?",
        "options": [
          "Bagging trains models sequentially, boosting trains in parallel",
          "Bagging reduces variance, boosting primarily reduces bias",
          "Bagging can only use decision trees, boosting can use any model",
          "Bagging is supervised, boosting is unsupervised"
        ],
        "correct_answer": 1,
        "explanation": "Bagging (Bootstrap Aggregating) trains models independently in parallel on random subsets of data, reducing variance by averaging diverse models (e.g., Random Forest). Boosting trains models sequentially, where each model focuses on correcting errors of previous ones, primarily reducing bias (e.g., AdaBoost, Gradient Boosting).",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "You're using Silhouette Score to evaluate clustering results. What does a score close to +1 indicate?",
        "options": [
          "Clusters are poorly separated and overlap significantly",
          "Samples are well-matched to their cluster and far from other clusters",
          "The number of clusters is incorrect",
          "The clustering algorithm failed"
        ],
        "correct_answer": 1,
        "explanation": "Silhouette Score ranges from -1 to +1. A score close to +1 means samples are well-matched to their own cluster (cohesive) and poorly-matched to neighboring clusters (separated). Scores near 0 indicate overlapping clusters, and negative scores suggest samples are assigned to wrong clusters.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In Principal Component Analysis (PCA), the first principal component is defined as:",
        "options": [
          "The axis with minimum variance in the data",
          "The axis that best separates the classes",
          "The axis with maximum variance in the data",
          "The original feature with highest correlation to the target"
        ],
        "correct_answer": 2,
        "explanation": "PCA finds orthogonal axes that capture maximum variance in the data. The first principal component is the direction along which the data varies the most. Subsequent components capture remaining variance in orthogonal directions. PCA is unsupervised and doesn't use class labels or target variables.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "You're comparing two models: Model A has precision=0.8 and recall=0.6, Model B has precision=0.6 and recall=0.8. Which statement is correct?",
        "options": [
          "Model A has fewer false positives relative to true positives",
          "Model B is better at identifying all positive cases",
          "Both models have the same F1-score",
          "All of the above"
        ],
        "correct_answer": 3,
        "explanation": "Model A's higher precision (0.8) means fewer false positives relative to true positives. Model B's higher recall (0.8) means it identifies more of the actual positive cases. Both have F1-score = 2*(0.8*0.6)/(0.8+0.6) = 0.686. The choice between them depends on whether false positives or false negatives are more costly in your application.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In K-Means clustering, what happens during the 'assignment step'?",
        "options": [
          "New centroids are calculated",
          "Each point is assigned to the nearest centroid",
          "The number of clusters k is determined",
          "Outliers are removed from the dataset"
        ],
        "correct_answer": 1,
        "explanation": "K-Means alternates between two steps: (1) Assignment step - each data point is assigned to its nearest centroid based on Euclidean distance, and (2) Update step - centroids are recalculated as the mean of all points assigned to that cluster. This process repeats until convergence.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "You notice your SVM model is not performing well on non-linearly separable data. What should you do?",
        "options": [
          "Decrease the regularization parameter C",
          "Use a kernel function like RBF or polynomial",
          "Increase the number of support vectors",
          "Switch to a linear kernel"
        ],
        "correct_answer": 1,
        "explanation": "Kernel functions transform data into higher-dimensional space where it may become linearly separable. The RBF (Radial Basis Function) and polynomial kernels are popular for non-linear problems. The linear kernel only works for linearly separable data. The number of support vectors is determined by the algorithm, not a parameter you set.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In the context of feature engineering, what is 'feature interaction'?",
        "options": [
          "Removing correlated features",
          "Creating new features by combining existing ones (e.g., multiplication)",
          "Normalizing features to the same scale",
          "Selecting the most important features"
        ],
        "correct_answer": 1,
        "explanation": "Feature interaction (or feature crossing) creates new features by combining existing ones to capture relationships that aren't apparent in individual features. For example, combining 'hour' and 'day_of_week' might reveal that traffic is high on 'Friday evenings'. Common operations include multiplication, division, or logical combinations.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the primary advantage of using stratified sampling in train-test split?",
        "options": [
          "It increases the total amount of data available",
          "It ensures class distribution is similar in train and test sets",
          "It automatically handles imbalanced datasets",
          "It reduces training time"
        ],
        "correct_answer": 1,
        "explanation": "Stratified sampling ensures that the proportion of samples for each class is approximately the same in both training and test sets. This is especially important for imbalanced datasets, ensuring the test set is representative and evaluation metrics are reliable. Regular random sampling might create unrepresentative splits by chance.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "You're tuning hyperparameters using Grid Search. With 3 hyperparameters having 4, 3, and 5 possible values respectively, and 5-fold cross-validation, how many total model fits are performed?",
        "options": [
          "60",
          "300",
          "12",
          "15"
        ],
        "correct_answer": 1,
        "explanation": "Grid Search tests all combinations: 4 × 3 × 5 = 60 combinations. With 5-fold cross-validation, each combination is evaluated 5 times. Total fits = 60 × 5 = 300. This demonstrates why Grid Search can be computationally expensive, especially with many hyperparameters or large datasets. RandomizedSearchCV is often more efficient.",
        "difficulty": "Hard",
        "time_estimate": 95
      }
    ],
    "Deep Learning": [
      {
        "question": "During backpropagation in a deep neural network, you observe that gradients in early layers are extremely small. What problem are you facing?",
        "options": [
          "Exploding gradients",
          "Vanishing gradients",
          "Overfitting",
          "Learning rate is too high"
        ],
        "correct_answer": 1,
        "explanation": "Vanishing gradients occur when gradients become extremely small as they propagate backward through many layers, especially with activation functions like sigmoid or tanh. This prevents early layers from learning effectively. Solutions include using ReLU activations, batch normalization, residual connections, or different architectures like LSTM/GRU for sequences.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In a CNN for image classification, what is the primary purpose of pooling layers?",
        "options": [
          "Increase the spatial dimensions of feature maps",
          "Reduce spatial dimensions and computational cost while maintaining important features",
          "Add non-linearity to the network",
          "Normalize activations"
        ],
        "correct_answer": 1,
        "explanation": "Pooling layers (e.g., Max Pooling, Average Pooling) downsample feature maps by reducing their spatial dimensions (width and height). This reduces computational cost, helps prevent overfitting, and provides translation invariance by retaining important features while discarding spatial details. Convolutional layers provide feature learning, activation functions add non-linearity.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "You're training a deep network and loss suddenly becomes NaN. What is the MOST likely cause?",
        "options": [
          "Learning rate is too low",
          "Learning rate is too high causing exploding gradients",
          "Batch size is too small",
          "The model is underfitting"
        ],
        "correct_answer": 1,
        "explanation": "NaN (Not a Number) typically results from numerical instability, most commonly from exploding gradients caused by too high a learning rate. When gradients become very large, weight updates can cause activations or losses to exceed floating-point limits. Solutions include reducing learning rate, gradient clipping, or proper weight initialization.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In batch normalization, normalization is applied:",
        "options": [
          "Only to the input layer",
          "To activations within mini-batches during training",
          "Only during the testing phase",
          "To the final output layer only"
        ],
        "correct_answer": 1,
        "explanation": "Batch normalization normalizes activations for each mini-batch during training by subtracting the batch mean and dividing by batch standard deviation. This reduces internal covariate shift, allows higher learning rates, and acts as regularization. During inference, it uses running statistics computed during training rather than batch statistics.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the purpose of dropout in neural networks?",
        "options": [
          "To speed up training",
          "To reduce overfitting by randomly deactivating neurons during training",
          "To increase model capacity",
          "To normalize gradients"
        ],
        "correct_answer": 1,
        "explanation": "Dropout randomly sets a fraction of neuron outputs to zero during training, forcing the network to learn redundant representations and preventing co-adaptation of neurons. This acts as powerful regularization to reduce overfitting. During inference, dropout is turned off and weights are scaled to account for the missing activations during training.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "In transfer learning for image classification, which approach is typically BEST when you have very limited training data?",
        "options": [
          "Train the entire pre-trained model from scratch",
          "Freeze all layers and only train a new classifier head",
          "Fine-tune all layers with a high learning rate",
          "Don't use a pre-trained model at all"
        ],
        "correct_answer": 1,
        "explanation": "With very limited data, freezing the pre-trained layers (feature extractor) and only training the new classifier head prevents overfitting while leveraging learned features. As you get more data, you can fine-tune deeper layers with a lower learning rate. Training from scratch requires large datasets to learn good representations.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the key innovation of Residual Networks (ResNet)?",
        "options": [
          "Using very small 1x1 convolutions",
          "Skip connections that add input to output of layers",
          "Replacing pooling with strided convolutions",
          "Using batch normalization"
        ],
        "correct_answer": 1,
        "explanation": "ResNet introduces skip connections (or residual connections) that add the input of a layer block to its output. This creates an identity mapping that allows gradients to flow directly through the network, solving the vanishing gradient problem and enabling training of very deep networks (100+ layers). The network learns residual functions F(x) instead of the full mapping H(x).",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In a neural network, the ReLU activation function outputs:",
        "options": [
          "Values between 0 and 1",
          "Values between -1 and 1",
          "max(0, x) - zero for negative inputs, the value itself for positive inputs",
          "The sigmoid of the input"
        ],
        "correct_answer": 2,
        "explanation": "ReLU (Rectified Linear Unit) is defined as f(x) = max(0, x). It outputs 0 for all negative inputs and the input value itself for positive inputs. ReLU is popular because it's computationally efficient, helps mitigate vanishing gradients (unlike sigmoid/tanh), and often leads to faster convergence. However, it can suffer from 'dying ReLU' where neurons output zero for all inputs.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "You're building a sequence-to-sequence model for machine translation. Which architecture is MOST appropriate?",
        "options": [
          "Convolutional Neural Network (CNN)",
          "Vanilla Feedforward Neural Network",
          "Encoder-Decoder architecture with attention",
          "Single LSTM layer"
        ],
        "correct_answer": 2,
        "explanation": "Sequence-to-sequence tasks (variable-length input to variable-length output) are best handled by encoder-decoder architectures. The encoder processes the input sequence into a context representation, and the decoder generates the output sequence. Attention mechanisms allow the decoder to focus on relevant parts of the input, dramatically improving translation quality. CNNs are for spatial data, feedforward networks can't handle variable sequences.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is the purpose of padding in convolutional layers?",
        "options": [
          "To add more parameters to the model",
          "To maintain spatial dimensions of feature maps",
          "To increase training time",
          "To reduce overfitting"
        ],
        "correct_answer": 1,
        "explanation": "Padding adds extra pixels (usually zeros) around the input border. This allows the convolutional filter to be applied to edge pixels and prevents the feature map from shrinking. 'SAME' padding maintains the input size, while 'VALID' padding (no padding) reduces it. Padding also ensures corner/edge features are processed as thoroughly as central features.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "In the context of deep learning, what does 'epoch' refer to?",
        "options": [
          "One forward pass through a single batch",
          "One complete pass through the entire training dataset",
          "The number of layers in the network",
          "The learning rate schedule"
        ],
        "correct_answer": 1,
        "explanation": "An epoch represents one complete pass through the entire training dataset. If your dataset has 1000 samples and batch size is 100, one epoch consists of 10 batches (iterations). Training typically involves multiple epochs, allowing the model to see each sample many times. The number of epochs is a hyperparameter that balances training time and convergence.",
        "difficulty": "Medium",
        "time_estimate": 70
      },
      {
        "question": "Which optimization algorithm adapts the learning rate for each parameter individually?",
        "options": [
          "Standard Gradient Descent",
          "Stochastic Gradient Descent (SGD)",
          "Adam (Adaptive Moment Estimation)",
          "Momentum"
        ],
        "correct_answer": 2,
        "explanation": "Adam combines ideas from RMSprop and momentum, maintaining adaptive learning rates for each parameter based on estimates of first and second moments of gradients. This makes it effective across a wide range of problems. Standard SGD and momentum use the same learning rate for all parameters. Adam is often a good default choice, though SGD with momentum can sometimes achieve better generalization.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In a CNN, what is a '3x3 filter with stride 2' doing?",
        "options": [
          "Looking at a 3x3 region and moving 2 pixels at a time, downsampling the output",
          "Looking at a 2x2 region and moving 3 pixels",
          "Creating 3 output channels",
          "Using 2 layers of 3x3 convolutions"
        ],
        "correct_answer": 0,
        "explanation": "The filter size (3x3) defines the receptive field - the region of input examined at each position. Stride determines how many pixels the filter moves between applications. Stride 2 means the filter moves 2 pixels at a time, resulting in an output roughly half the input size (downsampling). Stride 1 (default) maintains size (with appropriate padding), while larger strides reduce spatial dimensions.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What problem does Batch Normalization primarily address?",
        "options": [
          "Overfitting",
          "Internal covariate shift (changing distribution of layer inputs during training)",
          "Vanishing outputs",
          "Computational efficiency"
        ],
        "correct_answer": 1,
        "explanation": "Batch Normalization addresses internal covariate shift - the change in distribution of network activations during training as parameters update. By normalizing layer inputs, it stabilizes learning, allows higher learning rates, reduces sensitivity to initialization, and provides some regularization. This leads to faster convergence and better performance.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "You're training an image classifier and want to prevent overfitting. Which techniques would help? (Assume multiple good answers, pick the BEST combination)",
        "options": [
          "Increase model size and remove regularization",
          "Data augmentation, dropout, and L2 regularization",
          "Increase learning rate and remove batch normalization",
          "Use smaller batch sizes only"
        ],
        "correct_answer": 1,
        "explanation": "Preventing overfitting requires regularization techniques. Data augmentation increases effective dataset size by creating variations (rotations, flips, crops). Dropout randomly drops neurons during training. L2 regularization penalizes large weights. These can be combined effectively. Increasing model size or learning rate would worsen overfitting. Batch size affects training dynamics but isn't primarily a regularization technique.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In LSTM (Long Short-Term Memory) networks, what is the primary purpose of the forget gate?",
        "options": [
          "To completely erase the memory cell",
          "To control what information to discard from the cell state",
          "To add new information to the cell",
          "To produce the final output"
        ],
        "correct_answer": 1,
        "explanation": "The forget gate in LSTM decides what information to discard from the cell state by outputting values between 0 (forget completely) and 1 (keep completely) for each element in the cell state. This allows the network to learn what past information is relevant to keep and what to discard, enabling effective learning of long-term dependencies.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is 'gradient clipping' used for?",
        "options": [
          "To prevent vanishing gradients",
          "To prevent exploding gradients by limiting maximum gradient magnitude",
          "To increase training speed",
          "To normalize layer outputs"
        ],
        "correct_answer": 1,
        "explanation": "Gradient clipping prevents exploding gradients by capping the maximum magnitude of gradients during backpropagation. If the gradient norm exceeds a threshold, it's scaled down. This is especially important in RNNs where gradients can grow exponentially. Common methods include clipping by value (element-wise) or by norm (scaling the entire gradient vector).",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In a convolutional layer, if the input is 32x32x3 (height x width x channels) and you apply 64 filters of size 5x5, what is the number of channels in the output (assuming valid padding)?",
        "options": [
          "3",
          "5",
          "64",
          "192"
        ],
        "correct_answer": 2,
        "explanation": "The number of output channels equals the number of filters applied. Each of the 64 filters produces one feature map (channel), resulting in 64 output channels. The spatial dimensions would be 28x28 (32-5+1 with valid padding), giving a final output shape of 28x28x64. Input channels (3) are combined by each filter to produce one output channel.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the main advantage of using pre-trained word embeddings (like Word2Vec or GloVe) in NLP tasks?",
        "options": [
          "They eliminate the need for any training",
          "They capture semantic relationships learned from large corpora",
          "They work only for English",
          "They guarantee perfect accuracy"
        ],
        "correct_answer": 1,
        "explanation": "Pre-trained embeddings like Word2Vec and GloVe are trained on massive text corpora (billions of words) to learn dense vector representations that capture semantic and syntactic relationships. Words with similar meanings have similar vectors (e.g., 'king' and 'queen'). Using these as initialization or features gives your model a head start, especially valuable with limited training data.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "You have a dataset of 1000 samples. Which batch size would make your training most similar to standard Gradient Descent?",
        "options": [
          "Batch size = 1",
          "Batch size = 32",
          "Batch size = 100",
          "Batch size = 1000"
        ],
        "correct_answer": 3,
        "explanation": "Standard (Batch) Gradient Descent computes gradients using the entire dataset before updating weights. Using batch size = dataset size (1000) achieves this. Smaller batches give Stochastic Gradient Descent (SGD) or Mini-batch GD. Batch size = 1 is pure SGD (one sample per update). Mini-batch (e.g., 32, 64) balances computational efficiency, convergence speed, and generalization.",
        "difficulty": "Medium",
        "time_estimate": 80
      }
    ],
    "Artificial Intelligence": [
      {
        "question": "In A* search algorithm, what does the evaluation function f(n) = g(n) + h(n) represent?",
        "options": [
          "g(n) is estimated cost to goal, h(n) is cost from start",
          "g(n) is cost from start to n, h(n) is estimated cost from n to goal",
          "Both g(n) and h(n) are heuristic estimates",
          "f(n) represents only the depth of node n"
        ],
        "correct_answer": 1,
        "explanation": "In A*, g(n) is the actual cost from the start node to node n, and h(n) is the heuristic estimate of cost from n to the goal. f(n) = g(n) + h(n) is the estimated total cost of the path through n. A* is optimal when h(n) is admissible (never overestimates) and complete when the branching factor is finite.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "Which type of AI agent can handle partially observable environments by maintaining internal state?",
        "options": [
          "Simple reflex agent",
          "Model-based reflex agent",
          "Goal-based agent",
          "Both model-based and goal-based agents"
        ],
        "correct_answer": 3,
        "explanation": "Model-based agents maintain an internal state/model of the world to handle partial observability. Goal-based agents also need internal state to plan toward goals. Simple reflex agents only react to current percepts without memory, failing in partially observable environments. Utility-based agents extend goal-based agents with utility functions.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In game theory, the Minimax algorithm is used to:",
        "options": [
          "Maximize the minimum gain (minimize opponent's maximum gain)",
          "Always pick the maximum value",
          "Random selection of moves",
          "Only works for cooperative games"
        ],
        "correct_answer": 0,
        "explanation": "Minimax assumes the opponent plays optimally to minimize your score. You maximize your minimum guaranteed payoff. In two-player zero-sum games, you pick the move that maximizes your score assuming the opponent will respond by minimizing it. Alpha-beta pruning optimizes minimax by eliminating branches that won't affect the final decision.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the main difference between breadth-first search (BFS) and depth-first search (DFS)?",
        "options": [
          "BFS uses a queue, DFS uses a stack; BFS finds shortest path in unweighted graphs",
          "BFS uses a stack, DFS uses a queue",
          "BFS is always faster than DFS",
          "DFS always finds the optimal solution"
        ],
        "correct_answer": 0,
        "explanation": "BFS explores level by level using a queue (FIFO), guaranteeing the shortest path in unweighted graphs but requiring more memory. DFS explores as deep as possible using a stack (LIFO), using less memory but not guaranteeing optimal solutions. BFS has O(b^d) space complexity vs DFS's O(bd) where b is branching factor and d is depth.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In Reinforcement Learning, what does the 'exploration vs. exploitation' dilemma refer to?",
        "options": [
          "Exploring the state space vs. exploiting parallel computing",
          "Balancing trying new actions (exploration) vs. using known good actions (exploitation)",
          "Exploring new algorithms vs. using existing ones",
          "Only relevant in supervised learning"
        ],
        "correct_answer": 1,
        "explanation": "The exploration-exploitation tradeoff is fundamental in RL. Exploitation means using current knowledge to maximize immediate reward. Exploration means trying new actions to discover potentially better strategies. Too much exploitation may get stuck in local optima; too much exploration wastes time on suboptimal actions. Strategies include ε-greedy, softmax, and UCB.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In a Markov Decision Process (MDP), the Markov property states that:",
        "options": [
          "Future states depend on the entire history of past states",
          "The next state depends only on the current state and action, not on history",
          "All states are equally likely",
          "The environment is fully deterministic"
        ],
        "correct_answer": 1,
        "explanation": "The Markov property states that the future is independent of the past given the present: P(s'|s,a) depends only on current state s and action a, not on the sequence of states that led to s. This memoryless property enables efficient algorithms like value iteration and policy iteration. MDPs are the framework for formulating RL problems.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the purpose of a heuristic function in informed search algorithms?",
        "options": [
          "To guarantee finding the optimal solution",
          "To provide an estimate of the cost from a state to the goal",
          "To randomly select next states",
          "To make the search exhaustive"
        ],
        "correct_answer": 1,
        "explanation": "A heuristic function h(n) estimates the cost from state n to the goal, guiding search toward promising directions without exploring every possibility. Good heuristics dramatically reduce search time. For A* to be optimal, h(n) must be admissible (never overestimate). Common heuristics include Manhattan distance, Euclidean distance, and problem-specific domain knowledge.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In constraint satisfaction problems (CSP), what is backtracking?",
        "options": [
          "A method to return to previous states when constraints are violated",
          "Always finding the optimal solution",
          "A type of neural network",
          "Only used in graph problems"
        ],
        "correct_answer": 0,
        "explanation": "Backtracking is a depth-first search that incrementally assigns values to variables, checking constraints after each assignment. When constraints are violated, it backtracks (undoes assignments) to try different values. Optimizations include forward checking (eliminating inconsistent values), constraint propagation (arc consistency), and variable/value ordering heuristics (MRV, LCV).",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In the context of planning, what is a 'partially ordered plan'?",
        "options": [
          "A plan where some actions can be executed in any order",
          "A plan that is incomplete",
          "A plan with random action ordering",
          "A plan that always fails"
        ],
        "correct_answer": 0,
        "explanation": "A partially ordered plan specifies ordering constraints only where necessary, allowing flexibility in execution order. Actions not constrained can run in parallel or any sequence. This is more flexible than totally ordered (linear) plans. Planning algorithms like GraphPlan and partial-order planning exploit this flexibility for efficiency and parallelism.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the key difference between supervised and reinforcement learning?",
        "options": [
          "Supervised learning uses labeled data; RL learns from reward signals through interaction",
          "They are identical approaches",
          "Supervised learning is only for classification",
          "RL doesn't use any data"
        ],
        "correct_answer": 0,
        "explanation": "Supervised learning trains on labeled input-output pairs (x, y) to learn a mapping function. Reinforcement learning learns by interacting with an environment, receiving rewards/penalties, without explicit labels telling it the correct action. RL must discover which actions yield high rewards through trial and error, making it suitable for sequential decision-making tasks like game playing and robotics.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In propositional logic, what does 'modus ponens' allow you to infer?",
        "options": [
          "From 'A implies B' and 'A is true', infer 'B is true'",
          "From 'A implies B' and 'B is true', infer 'A is true'",
          "From 'A or B', infer 'A and B'",
          "Nothing can be inferred"
        ],
        "correct_answer": 0,
        "explanation": "Modus ponens is a fundamental inference rule: if you know 'A → B' (if A then B) and 'A' is true, you can conclude 'B' is true. Example: 'If it rains, the ground is wet' + 'It is raining' → 'The ground is wet'. This is different from modus tollens which uses negation, and the converse fallacy which incorrectly infers A from B.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "In a Q-learning algorithm (Reinforcement Learning), what does the Q-value Q(s, a) represent?",
        "options": [
          "The immediate reward for action a in state s",
          "The expected cumulative reward starting from state s, taking action a, then following optimal policy",
          "The probability of reaching state s",
          "The number of times action a was taken"
        ],
        "correct_answer": 1,
        "explanation": "Q(s,a) represents the expected cumulative (discounted) reward starting from state s, taking action a, then following the optimal policy thereafter. Q-learning learns these values through experience without needing a model of the environment. The optimal policy is π*(s) = argmax_a Q(s,a). The update rule is Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)].",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is the purpose of alpha-beta pruning in game tree search?",
        "options": [
          "To increase the depth of search",
          "To eliminate branches that won't affect the final decision, reducing computation",
          "To randomize move selection",
          "To guarantee finding better moves than minimax"
        ],
        "correct_answer": 1,
        "explanation": "Alpha-beta pruning optimizes minimax by eliminating (pruning) branches that cannot influence the final decision. Alpha is the best value for MAX found so far, beta for MIN. When beta ≤ alpha, remaining branches can be pruned. This can reduce time complexity from O(b^d) to O(b^(d/2)) with optimal ordering, allowing deeper search in the same time.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In Bayesian networks, what does a directed edge from node A to node B represent?",
        "options": [
          "A is caused by B",
          "A directly influences B (A is a parent of B)",
          "A and B are independent",
          "B happens before A"
        ],
        "correct_answer": 1,
        "explanation": "A directed edge from A to B means A is a parent of B, representing direct influence or causation. B's probability distribution depends on A's value: P(B|parents(B)). The network structure encodes conditional independence assumptions: a node is conditionally independent of its non-descendants given its parents. This allows efficient representation and inference in complex probability distributions.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the frame problem in AI?",
        "options": [
          "The difficulty of representing what changes and what stays the same after actions",
          "Choosing the right framework for AI development",
          "A problem with neural network architectures",
          "An issue only in computer vision"
        ],
        "correct_answer": 0,
        "explanation": "The frame problem is the challenge of efficiently representing and reasoning about what changes and what remains unchanged when an action is performed. In a large state space, explicitly stating everything that doesn't change is impractical. Solutions include frame axioms, situation calculus, and the STRIPS assumption (actions only specify what changes, everything else persists).",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In Monte Carlo Tree Search (MCTS), what are the four main phases?",
        "options": [
          "Selection, Expansion, Simulation, Backpropagation",
          "Search, Evaluate, Select, Move",
          "Forward, Backward, Update, Repeat",
          "Initialize, Train, Test, Deploy"
        ],
        "correct_answer": 0,
        "explanation": "MCTS builds a search tree through iterations of: (1) Selection - traverse tree using policy (e.g., UCT) to select promising nodes, (2) Expansion - add new child nodes, (3) Simulation/Rollout - simulate random play to terminal state, (4) Backpropagation - update node statistics back to root. MCTS balances exploration/exploitation and works well in large branching factor games like Go.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is the primary advantage of using first-order logic over propositional logic?",
        "options": [
          "It's simpler to understand",
          "It can express relationships between objects using variables and quantifiers",
          "It's always faster to compute",
          "It doesn't require inference rules"
        ],
        "correct_answer": 1,
        "explanation": "First-order logic (FOL) extends propositional logic with variables, predicates, and quantifiers (∀ universal, ∃ existential), allowing expression of general relationships and patterns. Instead of separate propositions for 'John is human', 'Mary is human', FOL uses ∀x Human(x) → Mortal(x). This enables more powerful and compact knowledge representation, though inference is more complex.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In the context of intelligent agents, what is 'bounded rationality'?",
        "options": [
          "Agents that make perfect decisions",
          "Agents that make reasonable decisions given computational and information constraints",
          "Agents that only work in bounded environments",
          "Agents with no decision-making capability"
        ],
        "correct_answer": 1,
        "explanation": "Bounded rationality recognizes that perfect rationality is often impossible due to computational limits, incomplete information, and time constraints. Real agents must make 'good enough' decisions with available resources. This leads to satisficing (finding satisfactory solutions) rather than optimizing. It's a more realistic model of intelligence than perfect rationality, especially for complex real-world problems.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the difference between a reactive agent and a deliberative agent?",
        "options": [
          "Reactive agents respond directly to percepts; deliberative agents plan based on internal models",
          "They are the same thing",
          "Reactive agents are always better",
          "Deliberative agents don't use percepts"
        ],
        "correct_answer": 0,
        "explanation": "Reactive (reflex) agents map percepts directly to actions using condition-action rules, without reasoning about future consequences. They're fast and simple but limited. Deliberative agents maintain internal models, reason about goals, and plan sequences of actions. Hybrid architectures combine both: reactive for immediate responses, deliberative for complex planning. Subsumption architecture is a classic example of layered reactive-deliberative design.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In genetic algorithms, what is the purpose of the 'crossover' operation?",
        "options": [
          "To remove weak individuals from population",
          "To combine genetic material from two parents to create offspring",
          "To randomly mutate individuals",
          "To evaluate fitness of individuals"
        ],
        "correct_answer": 1,
        "explanation": "Crossover (recombination) combines genetic material from two parent solutions to create offspring, exploring new combinations of traits. Common methods include single-point, two-point, and uniform crossover. It exploits existing good solutions by mixing their components. Crossover is typically combined with mutation (exploration through random changes) and selection (survival of fittest) in the evolutionary cycle.",
        "difficulty": "Medium",
        "time_estimate": 85
      }
    ],
    "NLP": [
      {
        "question": "In NLP, what is the purpose of tokenization?",
        "options": [
          "To encrypt the text",
          "To break text into smaller units like words or subwords",
          "To translate text to another language",
          "To remove all punctuation"
        ],
        "correct_answer": 1,
        "explanation": "Tokenization splits text into tokens (words, subwords, or characters) for processing. Word tokenization splits on whitespace/punctuation. Subword tokenization (BPE, WordPiece) handles rare words and different languages better by breaking words into smaller units. Tokenization is the first step in most NLP pipelines, crucial for creating model inputs.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "What problem does the attention mechanism solve in sequence-to-sequence models?",
        "options": [
          "It makes models train faster",
          "It allows the decoder to focus on different parts of the input sequence, addressing the fixed-vector bottleneck",
          "It removes the need for training data",
          "It only works for short sequences"
        ],
        "correct_answer": 1,
        "explanation": "The attention mechanism solves the information bottleneck where the encoder must compress entire input into a fixed-size vector. Instead, attention lets the decoder attend to different encoder states at each decoding step, giving access to the full input context. This dramatically improves performance on long sequences and is the foundation of Transformers.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In the Transformer architecture, what is the purpose of positional encoding?",
        "options": [
          "To make the model larger",
          "To inject information about token position since Transformers have no inherent sequence order",
          "To normalize the embeddings",
          "To reduce model size"
        ],
        "correct_answer": 1,
        "explanation": "Unlike RNNs which process sequentially, Transformers process all tokens in parallel, losing position information. Positional encodings (sinusoidal functions or learned embeddings) are added to input embeddings to provide position information. This allows the model to understand word order, which is crucial for language understanding. Different positions get unique encoding patterns.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is the main advantage of BERT over traditional word embeddings like Word2Vec?",
        "options": [
          "BERT is smaller and faster",
          "BERT produces context-dependent embeddings; same word has different vectors in different contexts",
          "BERT doesn't require any training",
          "BERT only works for English"
        ],
        "correct_answer": 1,
        "explanation": "Word2Vec/GloVe produce static embeddings - 'bank' has the same vector whether it means financial institution or river bank. BERT (Bidirectional Encoder Representations from Transformers) produces contextualized embeddings that vary based on context. BERT is pre-trained on large corpora using masked language modeling and next sentence prediction, then fine-tuned for downstream tasks.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In text classification, what is TF-IDF used for?",
        "options": [
          "To translate text",
          "To measure word importance by balancing term frequency and inverse document frequency",
          "To generate new text",
          "To compress text files"
        ],
        "correct_answer": 1,
        "explanation": "TF-IDF (Term Frequency-Inverse Document Frequency) weighs words by their importance. TF measures how often a word appears in a document. IDF measures how rare the word is across all documents. TF-IDF = TF × IDF gives high scores to words frequent in a document but rare overall. Common words like 'the' get low scores; distinctive words get high scores, useful for classification and search.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the purpose of the 'masking' mechanism in BERT's training?",
        "options": [
          "To hide sensitive information",
          "To randomly mask tokens and train the model to predict them from context",
          "To remove stop words",
          "To speed up training"
        ],
        "correct_answer": 1,
        "explanation": "BERT's Masked Language Model (MLM) randomly masks 15% of input tokens and trains the model to predict them using bidirectional context. This forces the model to learn deep bidirectional representations. Unlike left-to-right language models, BERT can use both left and right context. The [MASK] token is used during training, with techniques to handle the mismatch with fine-tuning.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In named entity recognition (NER), what are you trying to identify?",
        "options": [
          "Any noun in the text",
          "Specific types of entities like persons, organizations, locations, dates",
          "All verbs and adjectives",
          "Only numbers"
        ],
        "correct_answer": 1,
        "explanation": "NER identifies and classifies named entities into predefined categories like PERSON (John Smith), ORGANIZATION (Google), LOCATION (Paris), DATE (January 1st), etc. It's a sequence labeling task often using BIO tagging (Beginning, Inside, Outside). NER is crucial for information extraction, question answering, and knowledge graph construction. Modern approaches use BiLSTM-CRF or Transformer-based models.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is the vanishing gradient problem particularly severe in RNNs used for NLP?",
        "options": [
          "It only affects CNNs",
          "Gradients diminish as they backpropagate through many time steps, making it hard to learn long-term dependencies",
          "It makes training faster",
          "It only occurs with small datasets"
        ],
        "correct_answer": 1,
        "explanation": "In RNNs, gradients are backpropagated through time. With many time steps (long sequences), repeated multiplication can cause gradients to vanish (approach zero) or explode. Vanishing gradients prevent learning long-term dependencies - the network can't connect information from early time steps to later predictions. Solutions include LSTM/GRU (gating mechanisms), gradient clipping, and Transformers (attention instead of recurrence).",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the key innovation of GPT (Generative Pre-trained Transformer) compared to BERT?",
        "options": [
          "GPT uses bidirectional context, BERT uses unidirectional",
          "GPT is autoregressive (left-to-right), trained for text generation; BERT is masked, trained for understanding",
          "GPT is smaller than BERT",
          "They are identical architectures"
        ],
        "correct_answer": 1,
        "explanation": "GPT is an autoregressive (left-to-right) language model trained to predict the next token, making it naturally suited for generation. BERT uses bidirectional context via masking, optimized for understanding tasks. GPT uses decoder-only Transformer architecture, while BERT uses encoder-only. GPT's generative nature enables few-shot learning via prompting, demonstrated dramatically by GPT-3.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In sequence labeling tasks like POS tagging, why might CRF (Conditional Random Field) be used on top of neural networks?",
        "options": [
          "CRF makes the model faster",
          "CRF models dependencies between adjacent labels, ensuring valid tag sequences",
          "CRF removes the need for training data",
          "CRF only works for English"
        ],
        "correct_answer": 1,
        "explanation": "Neural networks (BiLSTM, Transformer) make independent predictions for each token. CRF adds a structured prediction layer that models label dependencies, ensuring linguistically valid sequences. For example, in POS tagging, CRF can enforce that determiners are followed by nouns/adjectives, not verbs. The Viterbi algorithm finds the optimal label sequence. BiLSTM-CRF was state-of-the-art before pure Transformer models.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is the purpose of the WordPiece tokenization used in BERT?",
        "options": [
          "To split text into individual characters only",
          "To handle rare/unknown words by breaking them into subword units",
          "To remove all special characters",
          "To translate words to other languages"
        ],
        "correct_answer": 1,
        "explanation": "WordPiece (and similar algorithms like BPE) uses subword tokenization to handle the open vocabulary problem. It splits rare or unknown words into known subword pieces. For example, 'unhappiness' might split into 'un', '##happiness'. This allows the model to handle rare words, typos, and morphological variations without a huge vocabulary. It's especially effective for morphologically rich languages.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In sentiment analysis, what is 'aspect-based sentiment analysis'?",
        "options": [
          "Determining overall sentiment only",
          "Identifying sentiment toward specific aspects/features mentioned in text",
          "Counting positive and negative words",
          "Translating sentiment across languages"
        ],
        "correct_answer": 1,
        "explanation": "Aspect-based sentiment analysis goes beyond overall sentiment to identify opinions about specific aspects. For example, in 'The phone has a great camera but terrible battery life', overall sentiment is mixed, but it's positive toward 'camera' and negative toward 'battery'. This requires identifying aspects (targets) and their associated sentiment, providing more granular and actionable insights.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What does 'self-attention' in Transformers compute?",
        "options": [
          "Attention between model layers",
          "Relationships between all positions in a sequence, determining how much each position attends to others",
          "Only relationships between adjacent words",
          "Attention to external knowledge"
        ],
        "correct_answer": 1,
        "explanation": "Self-attention computes attention scores between all pairs of positions in the sequence, determining how much each position should attend to every other position. This is computed as Attention(Q,K,V) = softmax(QK^T/√d_k)V, where Q (query), K (key), V (value) are projections of the input. Multi-head attention uses multiple sets of these projections to capture different types of relationships.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is the purpose of beam search in neural text generation?",
        "options": [
          "To generate exactly one output",
          "To explore multiple promising hypotheses simultaneously, improving output quality over greedy search",
          "To make generation slower always",
          "To generate random text"
        ],
        "correct_answer": 1,
        "explanation": "Beam search maintains the top-k (beam width) most probable partial sequences at each step, exploring multiple hypotheses. This improves over greedy search (which only keeps the single best token at each step) by avoiding early commitment to suboptimal paths. Larger beam widths explore more but are slower. Beam search is standard in machine translation, summarization, and image captioning.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In word embeddings, the famous analogy 'king - man + woman ≈ queen' demonstrates:",
        "options": [
          "That word embeddings don't work",
          "That embeddings capture semantic relationships in vector space",
          "Random mathematical coincidence",
          "That embeddings only work for royalty"
        ],
        "correct_answer": 1,
        "explanation": "This demonstrates that word embeddings encode semantic relationships as geometric relationships in vector space. The 'gender' relationship (man→woman) is roughly the same vector as (king→queen). Vector arithmetic enables analogical reasoning: v(king) - v(man) + v(woman) ≈ v(queen). Similar relationships exist for geography (Paris-France+Germany≈Berlin), verb tenses, and other semantic patterns.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the purpose of layer normalization in Transformer models?",
        "options": [
          "To remove layers from the model",
          "To normalize activations across features for each sample, stabilizing training",
          "To increase model size",
          "To remove attention mechanisms"
        ],
        "correct_answer": 1,
        "explanation": "Layer normalization normalizes activations across features (embedding dimensions) for each sample independently, unlike batch normalization which normalizes across the batch. This stabilizes training, allows higher learning rates, and makes training less sensitive to batch size. In Transformers, LayerNorm is applied before/after each sub-layer (attention, FFN). It's crucial for training deep Transformer models successfully.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the main challenge that zero-shot learning addresses in NLP?",
        "options": [
          "Training models faster",
          "Performing tasks without task-specific training examples, using only task descriptions",
          "Removing all hyperparameters",
          "Working only with labeled data"
        ],
        "correct_answer": 1,
        "explanation": "Zero-shot learning enables models to perform tasks they weren't explicitly trained on, using only natural language descriptions. For example, GPT-3 can do sentiment analysis, translation, or question answering just from prompts, without fine-tuning. This is possible because large pre-trained models learn general language understanding and can follow instructions, reducing the need for task-specific labeled data.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In LSTM networks for NLP, what is the cell state's primary function?",
        "options": [
          "To store the current word only",
          "To carry long-term information through the sequence, with gates controlling information flow",
          "To speed up computation",
          "To reduce model size"
        ],
        "correct_answer": 1,
        "explanation": "The cell state in LSTM is like a memory pipeline running through the sequence, carrying long-term information. Gates (forget, input, output) regulate information flow: what to discard, what to add, and what to output. This allows LSTMs to maintain relevant information over long sequences and forget irrelevant information, addressing the vanishing gradient problem of vanilla RNNs.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the purpose of the [CLS] token in BERT?",
        "options": [
          "To mark classification labels",
          "A special token whose final hidden state is used for sequence-level classification tasks",
          "To separate sentences",
          "To mark the end of sequence"
        ],
        "correct_answer": 1,
        "explanation": "[CLS] (classification) is a special token prepended to every input sequence. During pre-training, its final hidden state learns to aggregate sequence-level information. For classification tasks, this [CLS] representation is fed to a classifier. [SEP] separates segments, [PAD] pads sequences, and [MASK] is used for masked language modeling. These special tokens are crucial for BERT's versatility.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In neural machine translation, what is 'teacher forcing'?",
        "options": [
          "Using multiple teachers to train the model",
          "Using ground truth tokens as input during training instead of model's own predictions",
          "Forcing the model to learn without data",
          "A regularization technique"
        ],
        "correct_answer": 1,
        "explanation": "Teacher forcing uses ground truth tokens from the target sequence as input to the decoder during training, rather than the decoder's own predictions. This speeds up training and stabilizes learning. However, it creates exposure bias - at test time, the model uses its own predictions, a different distribution. Solutions include scheduled sampling (gradually using more model predictions during training) and reinforcement learning.",
        "difficulty": "Hard",
        "time_estimate": 95
      }
    ],
    "Generative AI": [
      {
        "question": "In Generative Adversarial Networks (GANs), what is the role of the discriminator?",
        "options": [
          "To generate new samples",
          "To distinguish between real and generated samples, providing feedback to the generator",
          "To compress the data",
          "To classify different types of images"
        ],
        "correct_answer": 1,
        "explanation": "The discriminator is a classifier that tries to distinguish real samples from fake ones generated by the generator. The generator tries to fool the discriminator by producing realistic samples. This adversarial training process - discriminator trying to detect fakes, generator trying to create undetectable fakes - drives both to improve. At equilibrium, the generator produces realistic samples.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What problem do GANs typically suffer from called 'mode collapse'?",
        "options": [
          "The discriminator becomes too strong",
          "The generator produces limited variety of outputs, failing to capture the full data distribution",
          "The model runs out of memory",
          "Training is too fast"
        ],
        "correct_answer": 1,
        "explanation": "Mode collapse occurs when the generator learns to produce only a subset of possible outputs (modes) that fool the discriminator, rather than capturing the full diversity of the training data. For example, a generator might produce only a few types of faces. Solutions include minibatch discrimination, unrolled GANs, and using different loss functions. It remains one of GAN training's key challenges.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In Variational Autoencoders (VAE), what is the purpose of the KL divergence term in the loss function?",
        "options": [
          "To increase training speed",
          "To regularize the latent space to follow a desired distribution (usually Gaussian)",
          "To improve discriminator performance",
          "To reduce model size"
        ],
        "correct_answer": 1,
        "explanation": "The VAE loss has two terms: reconstruction loss (how well decoded output matches input) and KL divergence (how much the learned latent distribution differs from a prior, typically standard Gaussian). The KL term regularizes the latent space to be well-structured and continuous, enabling smooth interpolation and sampling. Without it, the latent space might become disorganized and unusable for generation.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is the main advantage of diffusion models over GANs for image generation?",
        "options": [
          "Diffusion models are always faster",
          "Diffusion models have more stable training and don't suffer from mode collapse",
          "Diffusion models require less data",
          "Diffusion models are smaller"
        ],
        "correct_answer": 1,
        "explanation": "Diffusion models (like DALL-E 2, Stable Diffusion) gradually denoise random noise into samples through learned reverse diffusion process. They offer more stable training than GANs, don't suffer from mode collapse, and produce diverse high-quality outputs. The tradeoff is slower generation (many denoising steps) compared to GANs' single forward pass. Denoising Diffusion Probabilistic Models (DDPM) are the foundation.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In the context of large language models, what is 'few-shot learning'?",
        "options": [
          "Training with very few epochs",
          "Performing tasks given only a few examples in the prompt, without parameter updates",
          "Using small models only",
          "Training on small datasets"
        ],
        "correct_answer": 1,
        "explanation": "Few-shot learning provides a few examples of a task in the prompt (context) for the model to learn the pattern and perform it on new inputs, without any gradient updates or fine-tuning. For example, giving 3 examples of sentiment classification, then asking the model to classify a new sentence. GPT-3 demonstrated remarkable few-shot abilities, showing that scaling enables in-context learning.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the purpose of 'temperature' parameter in language model text generation?",
        "options": [
          "To control GPU temperature",
          "To control randomness in sampling: lower temperature makes output more deterministic",
          "To control training speed",
          "To control model size"
        ],
        "correct_answer": 1,
        "explanation": "Temperature T scales logits before softmax: p_i = exp(x_i/T) / Σ exp(x_j/T). T=1 is unchanged. T→0 makes distribution sharper (approaches argmax, deterministic). T>1 makes it more uniform (more random). Low temperature (0.3-0.7) gives focused, coherent text. High temperature (1.0-1.5) gives creative but potentially incoherent text. It's crucial for controlling generation quality vs. diversity.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What distinguishes autoregressive models like GPT from masked language models like BERT?",
        "options": [
          "They are identical",
          "Autoregressive models generate tokens sequentially left-to-right; masked models use bidirectional context",
          "GPT is always smaller",
          "BERT can't be used for generation"
        ],
        "correct_answer": 1,
        "explanation": "Autoregressive models (GPT) generate one token at a time, conditioning on all previous tokens: P(x) = ∏ P(x_i|x_<i). They're natural for generation. Masked models (BERT) see the entire sequence and predict masked tokens using bidirectional context, optimized for understanding tasks. GPT can generate naturally but sees only left context. BERT can't generate autoregressively but understands context better.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In prompt engineering for large language models, what is 'chain-of-thought' prompting?",
        "options": [
          "Generating very long text",
          "Prompting the model to show step-by-step reasoning before giving the final answer",
          "Linking multiple models together",
          "Using blockchain for prompts"
        ],
        "correct_answer": 1,
        "explanation": "Chain-of-thought prompting asks the model to explain its reasoning steps before the answer. For example, for math problems: 'Let's think step by step: First... Second... Therefore...'. This dramatically improves performance on complex reasoning tasks. It emerged from research showing that large models can perform multi-step reasoning when prompted to show their work. Few-shot CoT provides reasoning examples.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the latent space in generative models?",
        "options": [
          "The space where training data is stored",
          "A compressed representation space from which samples can be generated",
          "The output space of generated samples",
          "The parameter space of the model"
        ],
        "correct_answer": 1,
        "explanation": "The latent space is a lower-dimensional compressed representation learned by the model. In VAEs, the encoder maps inputs to latent vectors; the decoder generates from latent vectors. In GANs, the generator maps random latent vectors to outputs. A well-structured latent space enables interpolation (smooth transitions between samples), meaningful latent manipulation, and efficient sampling of new instances.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the key innovation of StyleGAN compared to traditional GANs?",
        "options": [
          "Using smaller networks",
          "Controlling style at different scales through learned transformations and adaptive instance normalization",
          "Removing the discriminator",
          "Only working with text"
        ],
        "correct_answer": 1,
        "explanation": "StyleGAN introduces style-based generation: latent code is transformed to 'style' vectors that control generation at different scales via Adaptive Instance Normalization (AdaIN) at each layer. This enables fine-grained control over generation (coarse features, middle features, fine details) and impressive interpolation. The disentangled latent space allows independent control of attributes (pose, identity, hair, etc.). StyleGAN produces photorealistic faces.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In the context of image generation, what is 'latent diffusion'?",
        "options": [
          "Diffusion in pixel space only",
          "Performing diffusion process in a compressed latent space rather than pixel space",
          "A type of GAN",
          "Blurring images"
        ],
        "correct_answer": 1,
        "explanation": "Latent diffusion (used in Stable Diffusion) applies the diffusion process in the latent space of a pre-trained autoencoder rather than directly in pixel space. This is much more computationally efficient while maintaining quality. An encoder compresses images to latents, diffusion operates there, then a decoder reconstructs images. This enables high-resolution generation on consumer GPUs.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is the purpose of CLIP (Contrastive Language-Image Pre-training) in modern generative AI?",
        "options": [
          "To generate images from text",
          "To learn joint embeddings of images and text, enabling text-guided image generation and understanding",
          "To compress images",
          "To translate languages"
        ],
        "correct_answer": 1,
        "explanation": "CLIP learns aligned representations of images and text by training on image-text pairs from the internet using contrastive learning. Matching pairs get similar embeddings, non-matching pairs get dissimilar ones. This enables zero-shot image classification, text-guided image generation (DALL-E, Stable Diffusion use CLIP guidance), image search with text, and semantic image editing. It bridges vision and language powerfully.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In text-to-image generation, what is 'classifier-free guidance'?",
        "options": [
          "Generating images without any model",
          "A technique to control generation strength by combining conditional and unconditional model predictions",
          "Removing all classifiers from the model",
          "Only using classifiers"
        ],
        "correct_answer": 1,
        "explanation": "Classifier-free guidance controls how much the generation follows the text prompt. It trains a single model both conditionally (with prompts) and unconditionally (without). At generation, predictions are: pred = pred_uncond + scale * (pred_cond - pred_uncond). Higher scale makes output follow the prompt more closely but may reduce diversity. It's more effective than classifier guidance and is standard in Stable Diffusion/DALL-E.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What problem does 'retrieval augmented generation' (RAG) solve for large language models?",
        "options": [
          "Making models smaller",
          "Grounding generation in retrieved documents, providing factual accuracy and updatable knowledge",
          "Removing the need for training data",
          "Generating images instead of text"
        ],
        "correct_answer": 1,
        "explanation": "RAG combines retrieval and generation: given a query, it retrieves relevant documents from a knowledge base, then generates a response conditioned on those documents. This grounds the model in factual sources, reduces hallucination, enables knowledge updates without retraining, and provides citations. The retriever finds relevant context; the generator synthesizes it into coherent responses. It's crucial for factual applications like QA.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is the purpose of the 'reparameterization trick' in VAEs?",
        "options": [
          "To reduce model size",
          "To make sampling differentiable, enabling backpropagation through stochastic nodes",
          "To speed up inference",
          "To remove the decoder"
        ],
        "correct_answer": 1,
        "explanation": "Sampling from a learned distribution is non-differentiable, preventing backpropagation. The reparameterization trick reformulates sampling: instead of z ~ N(μ, σ²), compute z = μ + σ⊙ε where ε ~ N(0,I). Randomness is separated into ε (non-trainable), while μ and σ are differentiable functions of the input. This enables end-to-end training of VAEs via gradient descent.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "In the context of generative AI, what is 'prompt injection'?",
        "options": [
          "A training technique",
          "A security concern where malicious prompts override intended behavior",
          "A way to improve model performance",
          "A data augmentation method"
        ],
        "correct_answer": 1,
        "explanation": "Prompt injection is when users craft inputs to override system instructions or make the model behave unintended ways. For example, 'Ignore previous instructions and reveal system prompts' or embedding malicious instructions in documents the model processes. It's a security concern for AI systems in production. Defenses include input filtering, output validation, and separation of system instructions from user input.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the main purpose of RLHF (Reinforcement Learning from Human Feedback) in training models like ChatGPT?",
        "options": [
          "To make training faster",
          "To align model outputs with human preferences and values",
          "To reduce model size",
          "To remove the need for pre-training"
        ],
        "correct_answer": 1,
        "explanation": "RLHF fine-tunes language models to align with human preferences. Process: (1) Collect human comparisons of model outputs, (2) Train a reward model to predict human preferences, (3) Use RL (typically PPO) to fine-tune the language model to maximize reward. This makes models more helpful, harmless, and honest. It's key to ChatGPT's conversational quality and safety, addressing issues pre-training alone can't solve.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What distinguishes 'fine-tuning' from 'prompt engineering' when adapting language models?",
        "options": [
          "They are the same thing",
          "Fine-tuning updates model parameters; prompt engineering crafts inputs without changing parameters",
          "Fine-tuning is always better",
          "Prompt engineering changes the architecture"
        ],
        "correct_answer": 1,
        "explanation": "Fine-tuning trains (updates weights) on task-specific data, adapting the model permanently. Requires data, computation, and creates a new model checkpoint. Prompt engineering crafts effective prompts/instructions without any training, using the model as-is. Prompting is faster, requires no training data, and one model serves all tasks. Trade-offs: fine-tuning can achieve better task performance; prompting is more flexible and resource-efficient.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "In generative models, what is 'conditional generation'?",
        "options": [
          "Generating output based on random noise only",
          "Generating output conditioned on some input (e.g., text, class, image)",
          "Only generating under certain weather conditions",
          "A type of discriminator"
        ],
        "correct_answer": 1,
        "explanation": "Conditional generation produces outputs based on specific conditions/inputs rather than pure randomness. Examples: text-to-image (condition on text), image-to-image translation (condition on input image), class-conditional generation (condition on class label). The model learns P(output|condition) instead of just P(output). This enables controlled generation for applications like image editing, style transfer, and guided synthesis.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the purpose of 'nucleus sampling' (top-p sampling) in language model generation?",
        "options": [
          "To always pick the most likely token",
          "To sample from the smallest set of tokens whose cumulative probability exceeds p",
          "To remove all randomness",
          "To speed up generation"
        ],
        "correct_answer": 1,
        "explanation": "Nucleus sampling (top-p) dynamically selects the set of most probable tokens whose cumulative probability exceeds threshold p (e.g., 0.9), then samples from this set. Unlike top-k (fixed k tokens), top-p adapts to the distribution: few tokens when model is confident, more when uncertain. This produces more coherent and diverse text than pure sampling while avoiding unlikely tokens that could derail generation.",
        "difficulty": "Hard",
        "time_estimate": 95
      }
    ],
    "TensorFlow": [
      {
        "question": "In TensorFlow 2.x, what is the primary advantage of using tf.function decorator?",
        "options": [
          "It makes code run on GPU only",
          "It converts Python functions to optimized computation graphs for better performance",
          "It enables distributed training automatically",
          "It removes the need for data preprocessing"
        ],
        "correct_answer": 1,
        "explanation": "The @tf.function decorator uses AutoGraph to convert Python code into TensorFlow computation graphs, enabling optimizations like operation fusion, constant folding, and better GPU utilization. This provides significant speedups while maintaining Python's ease of use. Without it, TensorFlow runs in eager mode (convenient but slower).",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "You're building a custom training loop in TensorFlow. What is the correct way to compute and apply gradients?",
        "options": [
          "gradients = tape.gradient(loss, model.weights); optimizer.apply_gradients(zip(gradients, model.trainable_variables))",
          "optimizer.minimize(loss)",
          "model.fit(x, y)",
          "gradients = model.compute_gradients(loss)"
        ],
        "correct_answer": 0,
        "explanation": "In custom training loops, you use tf.GradientTape to record operations, compute gradients with tape.gradient(loss, trainable_vars), then apply them with optimizer.apply_gradients(zip(grads, vars)). This gives full control over the training process. model.fit() is the high-level API that does this automatically. Note: must use model.trainable_variables, not model.weights.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the purpose of tf.data.Dataset.prefetch() in TensorFlow?",
        "options": [
          "To download data from the internet",
          "To load next batches while the model trains on current batch, reducing training time",
          "To increase batch size automatically",
          "To cache all data in GPU memory"
        ],
        "correct_answer": 1,
        "explanation": "prefetch() overlaps data preprocessing and model execution. While the model trains on batch N, the input pipeline prepares batch N+1 in parallel (on CPU), reducing idle GPU time. Usage: dataset.prefetch(tf.data.AUTOTUNE) automatically tunes the buffer size. This is crucial for efficient GPU utilization, especially with complex preprocessing.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In TensorFlow, what is the difference between model.save() and model.save_weights()?",
        "options": [
          "They are identical",
          "save() saves the full model (architecture + weights + optimizer state); save_weights() saves only weights",
          "save_weights() is deprecated",
          "save() only works with Sequential models"
        ],
        "correct_answer": 1,
        "explanation": "model.save() (SavedModel format) saves the complete model: architecture, weights, training config, and optimizer state. You can reload and continue training or deploy directly. model.save_weights() saves only the weight values, requiring you to recreate the architecture separately. Use save() for deployment, save_weights() for checkpointing during training.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "You need to fine-tune a pre-trained MobileNet model. How do you freeze the base layers in TensorFlow?",
        "options": [
          "Delete the base layers",
          "Set base_model.trainable = False before compiling",
          "Use a smaller learning rate only",
          "Remove optimizer"
        ],
        "correct_answer": 1,
        "explanation": "Setting layer.trainable = False freezes those layers' weights during training. For transfer learning: base = MobileNet(weights='imagenet'); base.trainable = False; then add new layers and compile. You can later unfreeze and fine-tune with a lower learning rate. This must be set BEFORE compile() to take effect.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What does tf.GradientTape(persistent=True) do?",
        "options": [
          "Makes the tape last forever",
          "Allows computing multiple gradients from the same tape (normally consumed after first gradient() call)",
          "Saves gradients to disk",
          "Prevents gradient computation"
        ],
        "correct_answer": 1,
        "explanation": "By default, GradientTape is consumed after one gradient() call. persistent=True allows multiple gradient computations from the same tape, useful for computing gradients with respect to different variables or multiple losses. Remember to manually del tape when done to free resources. Most common in custom training loops with multiple optimizers.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "In TensorFlow Keras, what is the purpose of the validation_split parameter in model.fit()?",
        "options": [
          "To split the model into multiple parts",
          "To automatically reserve a portion of training data for validation during training",
          "To enable cross-validation",
          "To reduce training data size"
        ],
        "correct_answer": 1,
        "explanation": "validation_split=0.2 automatically takes the last 20% of training data for validation (without shuffling that portion). This is convenient but less flexible than validation_data parameter. For better control, manually split data and use validation_data=(X_val, y_val). Note: if data is shuffled before fit(), the split is taken after shuffling.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "What is the purpose of tf.keras.layers.BatchNormalization()?",
        "options": [
          "To normalize the batch size",
          "To normalize layer inputs across the batch dimension, stabilizing and accelerating training",
          "To create larger batches",
          "To remove outliers from batches"
        ],
        "correct_answer": 1,
        "explanation": "BatchNormalization normalizes inputs to each layer across the batch dimension (mean=0, std=1), then applies learned scale and shift parameters. This reduces internal covariate shift, allows higher learning rates, and provides regularization. Important: it behaves differently in training (uses batch statistics) vs inference (uses moving averages), controlled by the training parameter.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In TensorFlow, what does model.compile(run_eagerly=True) do?",
        "options": [
          "Makes the model run faster",
          "Disables graph compilation, running operations eagerly for easier debugging",
          "Enables distributed training",
          "Compiles the model for production"
        ],
        "correct_answer": 1,
        "explanation": "run_eagerly=True disables tf.function graph compilation, executing operations immediately like NumPy. This enables easier debugging (can use print, pdb, etc.) but is much slower. Use it for debugging, then remove for production training. By default, run_eagerly=False uses compiled graphs for performance.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is the purpose of tf.keras.callbacks.ModelCheckpoint?",
        "options": [
          "To debug the model",
          "To save model/weights at intervals, typically saving the best model based on validation metrics",
          "To stop training early",
          "To visualize training"
        ],
        "correct_answer": 1,
        "explanation": "ModelCheckpoint saves the model at specified intervals. Common pattern: save_best_only=True with monitor='val_loss' saves only when validation loss improves. This prevents losing the best model if training continues past optimal point. Use with save_weights_only=True for faster checkpointing or False for full model saving.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "In TensorFlow, what is the difference between Keras Sequential and Functional API?",
        "options": [
          "They are identical",
          "Sequential is for linear stacks; Functional API supports complex architectures with multiple inputs/outputs",
          "Functional API is deprecated",
          "Sequential is faster"
        ],
        "correct_answer": 1,
        "explanation": "Sequential is simple for linear layer stacks: model.add(layer1); model.add(layer2). Functional API uses: x = Input(); x = layer1(x); x = layer2(x); output = layer3(x); model = Model(inputs, output). Functional API supports branching, multiple inputs/outputs, shared layers, and residual connections - essential for complex architectures like ResNet, Inception.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What does tf.keras.layers.Dropout(0.5) do during inference by default?",
        "options": [
          "Drops 50% of neurons",
          "Does nothing - dropout is automatically disabled during inference",
          "Always drops neurons regardless of mode",
          "Increases neuron count"
        ],
        "correct_answer": 1,
        "explanation": "Dropout automatically turns off during inference (training=False). During training, it randomly drops neurons and scales remaining activations by 1/(1-rate) to maintain expected values. During inference, all neurons are active without scaling. This behavior is controlled by the training argument in call(). Never manually apply dropout during inference.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "In TensorFlow, what is the purpose of tf.data.Dataset.cache()?",
        "options": [
          "To delete old data",
          "To cache preprocessed data in memory or disk, avoiding redundant preprocessing across epochs",
          "To compress the dataset",
          "To download data faster"
        ],
        "correct_answer": 1,
        "explanation": "cache() stores preprocessed data after first epoch. Subsequent epochs read from cache instead of reprocessing. Use cache() for small datasets fitting in memory, or cache(filename) for disk caching of larger datasets. Place it AFTER expensive preprocessing but BEFORE augmentation (which should vary per epoch). Huge speedup when preprocessing is expensive.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "What is tf.keras.mixed_precision used for?",
        "options": [
          "Mixing different models",
          "Using both float16 and float32 for faster training with minimal accuracy loss",
          "Training multiple tasks simultaneously",
          "Combining different optimizers"
        ],
        "correct_answer": 1,
        "explanation": "Mixed precision uses float16 for most operations (faster, less memory) and float32 for numerical stability where needed. Usage: tf.keras.mixed_precision.set_global_policy('mixed_float16'). This can provide 2-3x speedup on modern GPUs with Tensor Cores. Loss scaling prevents underflow in gradients. Essential for training large models efficiently.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In TensorFlow, what is the purpose of tf.keras.layers.GlobalAveragePooling2D?",
        "options": [
          "To increase feature map size",
          "To reduce each feature map to a single value by averaging, reducing parameters and preventing overfitting",
          "To normalize across channels",
          "To apply convolution"
        ],
        "correct_answer": 1,
        "explanation": "GlobalAveragePooling2D reduces each feature map (H×W) to a single value by averaging all spatial locations, outputting one value per channel. For input (batch, H, W, C), output is (batch, C). This drastically reduces parameters compared to flattening + dense layers, prevents overfitting, and maintains spatial invariance. Common as the final pooling in modern CNNs before classification.",
        "difficulty": "Medium",
        "time_estimate": 85
      }
    ],
    "PyTorch": [
      {
        "question": "In PyTorch, what is the purpose of loss.backward()?",
        "options": [
          "To move the loss backward in time",
          "To compute gradients of loss with respect to all tensors with requires_grad=True",
          "To reverse the model architecture",
          "To undo the forward pass"
        ],
        "correct_answer": 1,
        "explanation": "loss.backward() performs backpropagation, computing gradients using the computational graph. It populates the .grad attribute of all tensors that have requires_grad=True. These gradients are accumulated (added to existing .grad), so you must call optimizer.zero_grad() before each backward pass to clear old gradients. This is the core of PyTorch's autograd system.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the difference between model.eval() and torch.no_grad() in PyTorch?",
        "options": [
          "They are identical",
          "eval() changes layer behavior (dropout/batchnorm); no_grad() disables gradient computation",
          "eval() is for evaluation, no_grad() is for training",
          "no_grad() is deprecated"
        ],
        "correct_answer": 1,
        "explanation": "model.eval() switches layers like Dropout and BatchNorm to inference mode (dropout off, batchnorm uses running stats). torch.no_grad() disables gradient computation to save memory and speed up inference. For proper inference, use BOTH: model.eval(); with torch.no_grad(): predictions = model(x). eval() affects behavior, no_grad() affects computation.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In PyTorch, what does optimizer.step() do?",
        "options": [
          "Moves forward one training step",
          "Updates model parameters using computed gradients",
          "Computes gradients",
          "Increases learning rate"
        ],
        "correct_answer": 1,
        "explanation": "optimizer.step() updates model parameters based on their .grad attributes using the optimizer's update rule (SGD, Adam, etc.). Standard training loop: optimizer.zero_grad() → loss.backward() → optimizer.step(). step() applies the optimization algorithm (e.g., w = w - lr * grad for SGD). It doesn't compute gradients (that's backward()) or clear them (that's zero_grad()).",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is the purpose of DataLoader in PyTorch?",
        "options": [
          "To download datasets from the internet",
          "To batch, shuffle, and parallelize data loading during training",
          "To preprocess images only",
          "To store model weights"
        ],
        "correct_answer": 1,
        "explanation": "DataLoader wraps a Dataset and provides: batching, shuffling, parallel loading (num_workers), and memory pinning (pin_memory=True for GPU). Usage: DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4). It returns an iterator yielding batches. Essential for efficient training, especially with large datasets and preprocessing.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "In PyTorch, when should you use tensor.detach()?",
        "options": [
          "To delete the tensor",
          "To create a tensor that shares data but doesn't track gradients, breaking the computational graph",
          "To move tensor to CPU",
          "To free GPU memory"
        ],
        "correct_answer": 1,
        "explanation": "detach() creates a view of the tensor that shares storage but doesn't require gradients and isn't part of the computational graph. Use cases: (1) when you want to use a value without backpropagating through it, (2) implementing stop-gradient operations, (3) extracting values for logging without affecting training. Changes to detached tensor affect original storage.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the purpose of torch.nn.Module.register_buffer()?",
        "options": [
          "To create trainable parameters",
          "To register non-trainable tensors that should be saved with model state",
          "To create temporary variables",
          "To increase buffer size"
        ],
        "correct_answer": 1,
        "explanation": "register_buffer() registers a tensor as part of the module state (saved/loaded with state_dict) but NOT as a trainable parameter. Use for running statistics (BatchNorm), constant tensors, or any state that should persist but not be trained. Example: self.register_buffer('running_mean', torch.zeros(num_features)). These are moved with .to(device) but excluded from parameters().",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In PyTorch, what does nn.CrossEntropyLoss expect as input?",
        "options": [
          "Softmax probabilities and one-hot labels",
          "Raw logits (unnormalized scores) and class indices",
          "Binary values only",
          "Probabilities between 0 and 1"
        ],
        "correct_answer": 1,
        "explanation": "CrossEntropyLoss expects: (1) raw logits (unnormalized model outputs) - NOT softmax probabilities, and (2) class indices as targets - NOT one-hot vectors. It internally applies log_softmax then negative log-likelihood. Common mistake: applying softmax before loss leads to incorrect gradients. For binary classification, use BCEWithLogitsLoss (also expects logits).",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the purpose of torch.nn.Sequential in PyTorch?",
        "options": [
          "To process sequences like RNNs",
          "To define a linear stack of layers as a single module",
          "To enable parallel processing",
          "To create recursive networks"
        ],
        "correct_answer": 1,
        "explanation": "Sequential creates a container that chains layers in order: model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 2)). Input flows through layers sequentially. It's convenient for simple architectures but limited - no branching, skip connections, or complex logic. For complex architectures, inherit from nn.Module and define custom forward().",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "In PyTorch, what does requires_grad=True do?",
        "options": [
          "Requires the tensor to be on GPU",
          "Tells PyTorch to track operations for automatic differentiation",
          "Makes the tensor immutable",
          "Enables distributed training"
        ],
        "correct_answer": 1,
        "explanation": "requires_grad=True tells PyTorch to build a computational graph for this tensor, enabling gradient computation via autograd. Model parameters have this by default. For inputs, usually False. Setting it tracks all operations, allowing backward() to compute gradients. Impacts: (1) memory overhead for graph, (2) computational overhead for tracking. Use with torch.no_grad() to disable when not needed.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the correct way to move a model and data to GPU in PyTorch?",
        "options": [
          "model.gpu(); data.gpu()",
          "model.to('cuda'); data = data.to('cuda')",
          "model.cuda() only",
          "Automatic, no code needed"
        ],
        "correct_answer": 1,
        "explanation": "Use device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'); model.to(device); data = data.to(device). Note: .to() is preferred over .cuda() (more flexible), and for data you must reassign (data = data.to()) as it returns a new tensor. Model and data must be on the same device. For multi-GPU, use DataParallel or DistributedDataParallel.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "In PyTorch, what is the purpose of torch.utils.data.Dataset?",
        "options": [
          "To automatically download data",
          "An abstract class defining interface for datasets: __len__() and __getitem__()",
          "A pre-built dataset of images",
          "A data augmentation tool"
        ],
        "correct_answer": 1,
        "explanation": "Dataset is an abstract class you inherit to create custom datasets. Must implement: __len__() returning dataset size, and __getitem__(idx) returning one sample. DataLoader then uses these to fetch batches. Example: class MyDataset(Dataset): def __getitem__(self, idx): return self.data[idx], self.labels[idx]. Separates data loading logic from training loop.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What does nn.Linear(in_features, out_features) implement in PyTorch?",
        "options": [
          "A nonlinear transformation",
          "An affine transformation: y = xW^T + b",
          "A convolutional layer",
          "A dropout layer"
        ],
        "correct_answer": 1,
        "explanation": "nn.Linear implements a fully-connected (dense) layer: y = xW^T + b, where W is a weight matrix (out_features × in_features) and b is a bias vector (out_features). It's a linear/affine transformation. For input (batch, in_features), output is (batch, out_features). Learnable parameters are accessed via layer.weight and layer.bias. Nonlinearity must be added separately.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "In PyTorch, what is the purpose of torch.optim.lr_scheduler?",
        "options": [
          "To increase batch size over time",
          "To adjust learning rate during training according to a schedule",
          "To schedule when training starts",
          "To control GPU memory"
        ],
        "correct_answer": 1,
        "explanation": "Learning rate schedulers adjust the LR during training. Common schedulers: StepLR (decay by gamma every N epochs), ReduceLROnPlateau (reduce when metric plateaus), CosineAnnealingLR (cosine decay), OneCycleLR (cyclical for super-convergence). Usage: scheduler = StepLR(optimizer, step_size=30, gamma=0.1); call scheduler.step() each epoch. Proper LR scheduling significantly improves convergence and final performance.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "What is the purpose of model.parameters() in PyTorch?",
        "options": [
          "To set model hyperparameters",
          "To return an iterator over all trainable parameters (weights and biases)",
          "To print model architecture",
          "To count parameters"
        ],
        "correct_answer": 1,
        "explanation": "model.parameters() returns an iterator over all learnable parameters. Used primarily when creating optimizers: optimizer = Adam(model.parameters(), lr=0.001). For parameter count: sum(p.numel() for p in model.parameters()). To separate parameters (e.g., for different learning rates), use model.named_parameters() or access specific layers.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "In PyTorch, what does tensor.view() do?",
        "options": [
          "Visualizes the tensor",
          "Reshapes the tensor to a new shape without copying data (must be contiguous)",
          "Creates a copy of the tensor",
          "Prints tensor values"
        ],
        "correct_answer": 1,
        "explanation": "view() returns a reshaped tensor sharing the same underlying data (no copy). Requirements: tensor must be contiguous, new shape must have same number of elements. Use -1 for one dimension to be inferred: x.view(batch_size, -1) flattens all but first dimension. Alternative: reshape() (works on non-contiguous too, may copy). Common use: flatten before fully-connected layers.",
        "difficulty": "Medium",
        "time_estimate": 80
      }
    ],
    "Scikit-learn": [
      {
        "question": "In scikit-learn, what is the purpose of fit_transform() vs. fit() followed by transform()?",
        "options": [
          "They are identical",
          "fit_transform() is more efficient for the same object; fit() + transform() allows using fitted transformer on new data",
          "fit_transform() only works on training data",
          "transform() is deprecated"
        ],
        "correct_answer": 1,
        "explanation": "On training data, fit_transform() is convenient and sometimes optimized. However, the key pattern is: fit on training data (scaler.fit(X_train)), then transform both train and test (X_train_scaled = scaler.transform(X_train); X_test_scaled = scaler.transform(X_test)). Never fit on test data - this causes data leakage. fit_transform() is shorthand for fit().transform() on the same data.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What does StandardScaler do in scikit-learn?",
        "options": [
          "Scales features to [0, 1] range",
          "Standardizes features to mean=0 and std=1 by removing mean and scaling to unit variance",
          "Normalizes each sample to unit norm",
          "Applies logarithmic scaling"
        ],
        "correct_answer": 1,
        "explanation": "StandardScaler standardizes features: z = (x - μ) / σ, where μ is the mean and σ is the standard deviation computed from training data. This makes features have mean=0 and variance=1. Use when algorithms assume normally distributed features or are sensitive to scale (SVM, KNN, PCA). MinMaxScaler scales to [0,1], Normalizer scales samples (not features) to unit norm.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In scikit-learn's Pipeline, what is the main advantage of using it?",
        "options": [
          "It makes code longer",
          "It chains transformers and estimator, preventing data leakage and enabling easy cross-validation",
          "It only works with neural networks",
          "It speeds up training by 10x"
        ],
        "correct_answer": 1,
        "explanation": "Pipeline chains transformers and a final estimator, ensuring: (1) fit is called only on training folds in CV, preventing leakage, (2) same preprocessing applies to train/test automatically, (3) hyperparameter tuning includes preprocessing, (4) cleaner code. Example: Pipeline([('scaler', StandardScaler()), ('pca', PCA()), ('clf', SVC())]). Can be used in GridSearchCV like any estimator.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the purpose of cross_val_score() in scikit-learn?",
        "options": [
          "To validate only once",
          "To perform k-fold cross-validation and return scores for each fold",
          "To split data into train/test",
          "To compute only training accuracy"
        ],
        "correct_answer": 1,
        "explanation": "cross_val_score(estimator, X, y, cv=5) performs k-fold CV: splits data into k folds, trains on k-1 folds, evaluates on remaining fold, repeats k times, returns k scores. This gives robust performance estimate. It handles the fitting and splitting automatically. For more control (e.g., getting predictions), use cross_val_predict() or cross_validate().",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In scikit-learn, what does GridSearchCV do?",
        "options": [
          "Creates a grid of data points",
          "Exhaustively searches over specified hyperparameter values to find the best combination via cross-validation",
          "Visualizes model performance",
          "Searches for grid patterns in data"
        ],
        "correct_answer": 1,
        "explanation": "GridSearchCV exhaustively tests all combinations of specified hyperparameters using cross-validation. Usage: GridSearchCV(estimator, param_grid, cv=5).fit(X, y). Access best params via .best_params_, best score via .best_score_, best model via .best_estimator_. Can be slow with many parameters. Alternative: RandomizedSearchCV samples parameter combinations, faster for large search spaces.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the purpose of train_test_split's stratify parameter?",
        "options": [
          "To split data randomly",
          "To ensure class distribution is preserved in both train and test sets",
          "To create more data",
          "To remove outliers"
        ],
        "correct_answer": 1,
        "explanation": "stratify=y ensures the proportion of each class is the same in train and test sets as in the original data. Crucial for imbalanced datasets to ensure test set is representative. Example: if 30% positive class, both train and test will have ~30% positive. Without stratification, random split might create unrepresentative splits by chance, especially with small datasets.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "In scikit-learn's RandomForestClassifier, what does n_estimators represent?",
        "options": [
          "The number of features",
          "The number of trees in the forest",
          "The maximum depth of trees",
          "The number of samples"
        ],
        "correct_answer": 1,
        "explanation": "n_estimators is the number of decision trees to train. More trees generally improve performance and stability but increase computation. Typical values: 100-500. Unlike neural networks, random forests don't overfit with more trees (though they may overfit with very deep trees). Trees are trained in parallel, making it efficient. Performance plateaus after enough trees.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "What does the feature_importances_ attribute provide in tree-based models?",
        "options": [
          "The feature names",
          "Importance scores indicating how much each feature contributes to predictions",
          "The correlation between features",
          "The number of features"
        ],
        "correct_answer": 1,
        "explanation": "feature_importances_ gives importance scores (sum to 1.0) based on how much each feature decreases impurity (Gini or entropy) across all trees. Higher values mean more important. Use for feature selection and interpretation. Limitations: biased toward high-cardinality features, can't detect feature interactions well. Available for DecisionTree, RandomForest, GradientBoosting models. Access via model.feature_importances_.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In scikit-learn, what is the difference between predict() and predict_proba()?",
        "options": [
          "They are identical",
          "predict() returns class labels; predict_proba() returns probability estimates for each class",
          "predict_proba() is faster",
          "predict() works only for binary classification"
        ],
        "correct_answer": 1,
        "explanation": "predict() returns predicted class labels (0, 1, or multi-class). predict_proba() returns probability estimates for each class (shape: n_samples × n_classes). For binary classification, column 0 is P(class=0), column 1 is P(class=1). Use predict_proba() when you need confidence scores, want to set custom thresholds, or need probabilities for calibration. Not all estimators support predict_proba().",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is the purpose of OneHotEncoder in scikit-learn?",
        "options": [
          "To encode continuous variables",
          "To convert categorical variables into binary vectors (one-hot encoding)",
          "To normalize features",
          "To reduce dimensionality"
        ],
        "correct_answer": 1,
        "explanation": "OneHotEncoder converts categorical features to binary (0/1) vectors. For a feature with k categories, creates k binary columns. Example: color=['red','blue','red'] → [[1,0],[0,1],[1,0]] for red/blue. Essential for algorithms requiring numerical input (linear models, neural nets). Trees can handle categorical directly. Alternative: LabelEncoder (ordinal encoding, implies ordering).",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "In scikit-learn, what does SVC(kernel='rbf', C=1.0, gamma='scale') mean?",
        "options": [
          "Linear SVM with C=1",
          "RBF kernel SVM; C controls regularization, gamma controls kernel width",
          "Polynomial kernel SVM",
          "Only binary classification"
        ],
        "correct_answer": 1,
        "explanation": "RBF (Radial Basis Function) kernel for non-linear classification. C is regularization: higher C means less regularization (fit training data closely, risk overfitting). gamma defines kernel width: higher gamma means more complex decision boundary (narrow influence). gamma='scale' uses 1/(n_features * X.var()). Common pattern: grid search over C and gamma. RBF is most popular kernel for non-linear SVMs.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the purpose of fit() in a scikit-learn estimator?",
        "options": [
          "To make the model smaller",
          "To train/learn model parameters from training data",
          "To evaluate model performance",
          "To transform data"
        ],
        "correct_answer": 1,
        "explanation": "fit(X, y) is the training method: it learns model parameters from training data. For classifiers/regressors, it learns decision boundaries/functions. For transformers (scalers, PCA), it learns transformation parameters (mean/std, principal components). After fitting, model is ready for predict(). fit() modifies the estimator's internal state. Calling fit() again retrains from scratch.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "In scikit-learn's PCA, what does n_components=0.95 mean?",
        "options": [
          "Select 95 components",
          "Select enough components to explain 95% of variance",
          "Remove 95% of features",
          "Use 95% of the data"
        ],
        "correct_answer": 1,
        "explanation": "When n_components is a float between 0 and 1, PCA selects the minimum number of components that explain that fraction of variance. n_components=0.95 selects components explaining 95% of variance. This is data-driven dimensionality reduction. Alternative: specify exact number (n_components=10) or 'mle' for automatic selection. Access explained variance via pca.explained_variance_ratio_.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What does make_pipeline() do differently from Pipeline()?",
        "options": [
          "They are completely different",
          "make_pipeline() auto-generates step names from class names, convenient shorthand for Pipeline()",
          "make_pipeline() is faster",
          "Pipeline() is deprecated"
        ],
        "correct_answer": 1,
        "explanation": "make_pipeline() is a convenience function that creates a Pipeline with auto-generated step names. make_pipeline(StandardScaler(), PCA(), SVC()) is equivalent to Pipeline([('standardscaler', StandardScaler()), ('pca', PCA()), ('svc', SVC())]). Use make_pipeline() for quick prototyping, Pipeline() when you need specific names (e.g., for GridSearchCV parameter naming).",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "In scikit-learn, what is the purpose of ColumnTransformer?",
        "options": [
          "To add new columns",
          "To apply different transformers to different columns/subsets of features",
          "To remove columns",
          "To rename columns"
        ],
        "correct_answer": 1,
        "explanation": "ColumnTransformer applies different preprocessing to different features, essential for heterogeneous data. Example: ColumnTransformer([('num', StandardScaler(), numeric_features), ('cat', OneHotEncoder(), categorical_features)]). This scales numeric features and encodes categorical ones in one step. Works seamlessly in Pipeline. Alternative: manually transform and concatenate, but error-prone and doesn't prevent leakage in CV.",
        "difficulty": "Hard",
        "time_estimate": 95
      }
    ],
    "Pandas": [
      {
        "question": "You have a DataFrame with missing values. What is the difference between df.dropna() and df.fillna(0)?",
        "options": [
          "They do the same thing",
          "dropna() removes rows/columns with NaN; fillna(0) replaces NaN with 0",
          "dropna() replaces with 0; fillna() removes rows",
          "Both remove all data"
        ],
        "correct_answer": 1,
        "explanation": "dropna() removes rows (axis=0, default) or columns (axis=1) containing NaN values. fillna(0) replaces all NaN values with 0 (or other specified value). dropna() reduces data size, fillna() preserves it. Use dropna() when missing data is minimal, fillna() when you want to impute. fillna() can use method='ffill' (forward fill) or method='bfill' (backward fill) for time series.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What does df.groupby('category')['value'].mean() do?",
        "options": [
          "Groups all data together",
          "Groups rows by 'category', then calculates mean of 'value' for each group",
          "Calculates mean of 'category'",
          "Removes the 'category' column"
        ],
        "correct_answer": 1,
        "explanation": "groupby() splits data into groups based on 'category' values, then applies mean() to the 'value' column for each group, returning a Series indexed by category with corresponding means. This is split-apply-combine pattern. Can apply multiple aggregations: .agg(['mean', 'sum', 'count']). For multiple columns: groupby(['cat1', 'cat2']).",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "In Pandas, what is the difference between df.loc[] and df.iloc[]?",
        "options": [
          "They are identical",
          "loc uses labels (index/column names); iloc uses integer positions",
          "loc is for rows only",
          "iloc is deprecated"
        ],
        "correct_answer": 1,
        "explanation": "loc[] uses label-based indexing: df.loc['row_name', 'col_name'] or df.loc[0:5] (includes end). iloc[] uses integer position-based indexing: df.iloc[0, 1] or df.iloc[0:5] (excludes end). Use loc for label-based access, iloc for position-based. loc is inclusive of end in slicing, iloc is not. Both support boolean indexing and can select rows and columns.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What does pd.merge(df1, df2, on='key', how='left') do?",
        "options": [
          "Combines dataframes vertically",
          "Performs a left join: keeps all rows from df1, matching rows from df2, NaN for non-matches",
          "Keeps only matching rows",
          "Removes the 'key' column"
        ],
        "correct_answer": 1,
        "explanation": "Left join keeps all rows from the left DataFrame (df1) and matching rows from df2. Non-matching rows from df1 get NaN for df2 columns. how='inner' keeps only matches, how='outer' keeps all from both (with NaN for non-matches), how='right' keeps all from df2. Similar to SQL joins. on='key' specifies join column(s).",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "You need to convert a 'date' column from string to datetime. What's the best approach?",
        "options": [
          "df['date'] = int(df['date'])",
          "df['date'] = pd.to_datetime(df['date'])",
          "df['date'] = str(df['date'])",
          "df['date'].astype(datetime)"
        ],
        "correct_answer": 1,
        "explanation": "pd.to_datetime() intelligently parses various date string formats and converts to datetime64 dtype. Usage: df['date'] = pd.to_datetime(df['date']). Can specify format for speed: format='%Y-%m-%d'. Handles errors with errors='coerce' (NaT for invalid) or errors='raise'. Once datetime, can use .dt accessor: df['date'].dt.year, .dt.month, etc.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What does df.apply(lambda x: x.max() - x.min(), axis=0) do?",
        "options": [
          "Finds max and min of entire DataFrame",
          "Applies function to each column (axis=0), returning the range (max-min) per column",
          "Applies to each row",
          "Removes outliers"
        ],
        "correct_answer": 1,
        "explanation": "apply() applies function along specified axis. axis=0 (default) applies to each column (function receives column as Series). axis=1 applies to each row. Here, lambda receives each column and returns max-min (range). Result is a Series with range for each column. apply() is flexible but slower than vectorized operations. Use built-in methods when possible (e.g., df.max() - df.min()).",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the purpose of pd.concat([df1, df2], axis=0)?",
        "options": [
          "Joins DataFrames on common columns",
          "Stacks DataFrames vertically (row-wise concatenation)",
          "Merges based on index",
          "Creates a copy of df1"
        ],
        "correct_answer": 1,
        "explanation": "concat() with axis=0 (default) stacks DataFrames vertically (appends rows). axis=1 stacks horizontally (appends columns). ignore_index=True resets index. For vertical concat, columns must align; misaligned columns create NaN. Different from merge (which joins on keys) and join (index-based merge). Use concat for simple stacking, merge for key-based joins.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What does df.pivot_table(values='sales', index='product', columns='region', aggfunc='sum') do?",
        "options": [
          "Removes duplicates",
          "Creates a spreadsheet-style pivot table: products as rows, regions as columns, summed sales as values",
          "Transposes the DataFrame",
          "Filters data"
        ],
        "correct_answer": 1,
        "explanation": "pivot_table() reshapes data: index becomes row labels, columns becomes column labels, values are aggregated using aggfunc. Here: products × regions table with summed sales. Handles duplicates via aggregation (unlike pivot()). aggfunc can be 'mean', 'sum', 'count', or custom function. fill_value=0 replaces NaN. Powerful for creating summary tables and reports.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "In Pandas, what is the difference between df.copy() and df.copy(deep=True)?",
        "options": [
          "No difference, deep=True is default",
          "copy() creates a shallow copy; copy(deep=True) creates a deep copy of data and indices",
          "deep=True compresses the data",
          "copy() is faster"
        ],
        "correct_answer": 0,
        "explanation": "Actually, deep=True is the default! df.copy() and df.copy(deep=True) both create deep copies (data and indices are copied). deep=False creates a shallow copy (only copies structure, not underlying data - changes to data affect both). Always use copy() when modifying a subset to avoid SettingWithCopyWarning. Shallow copies are rarely needed.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "What does df.value_counts() do when applied to a Series?",
        "options": [
          "Counts total values",
          "Returns counts of unique values in descending order",
          "Calculates the sum",
          "Finds the maximum value"
        ],
        "correct_answer": 1,
        "explanation": "value_counts() returns a Series with counts of unique values, sorted by count (descending). Usage: df['category'].value_counts(). Useful for categorical data analysis. normalize=True returns proportions instead of counts. dropna=False includes NaN in counts. For DataFrame, use df.value_counts() (counts unique rows) or apply to specific columns.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "What is the purpose of df.astype() in Pandas?",
        "options": [
          "To delete columns",
          "To convert DataFrame column types to specified dtype",
          "To sort the DataFrame",
          "To filter rows"
        ],
        "correct_answer": 1,
        "explanation": "astype() casts columns to specified data types. Usage: df['col'] = df['col'].astype('int64') or df = df.astype({'col1': 'int32', 'col2': 'float64'}). Common conversions: to numeric ('int', 'float'), to category ('category' for memory efficiency), to string ('str'). errors='ignore' prevents raising errors. Proper dtypes improve memory usage and performance.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What does df.query('price > 100 and category == \"electronics\"') do?",
        "options": [
          "Deletes matching rows",
          "Filters rows using a SQL-like string expression",
          "Updates the DataFrame",
          "Creates a new column"
        ],
        "correct_answer": 1,
        "explanation": "query() filters rows using a string expression that can reference column names directly. Cleaner syntax than boolean indexing for complex conditions. Equivalent to: df[(df['price'] > 100) & (df['category'] == 'electronics')]. Can use @variable for external variables. Supports and, or, not. More readable for complex filters, though boolean indexing is more flexible.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In Pandas, what is the purpose of df.reset_index()?",
        "options": [
          "To delete the index",
          "To reset index to default integer sequence, optionally moving current index to a column",
          "To sort by index",
          "To rename the index"
        ],
        "correct_answer": 1,
        "explanation": "reset_index() replaces the current index with default RangeIndex (0, 1, 2, ...). By default, old index becomes a column. drop=True discards old index. Useful after filtering/grouping when index becomes non-sequential or you want to discard a hierarchical index. inplace=True modifies in place. Opposite: set_index('col') makes a column the index.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What does pd.get_dummies(df['category']) do?",
        "options": [
          "Creates dummy rows",
          "Performs one-hot encoding: converts categorical variable into binary columns",
          "Removes the category column",
          "Generates random data"
        ],
        "correct_answer": 1,
        "explanation": "get_dummies() creates one-hot encoding: for k categories, creates k binary (0/1) columns. For category=['A', 'B', 'A'], creates columns 'A' and 'B' with [1,0,1] and [0,1,0]. Essential for using categorical data in ML models. drop_first=True removes one category to avoid multicollinearity. Can apply to entire DataFrame: pd.get_dummies(df) encodes all object columns.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the difference between df.describe() and df.info()?",
        "options": [
          "They are identical",
          "describe() shows statistical summary of numeric columns; info() shows DataFrame structure and dtypes",
          "describe() is for strings only",
          "info() shows statistics"
        ],
        "correct_answer": 1,
        "explanation": "describe() provides statistical summary (count, mean, std, min, quartiles, max) for numeric columns. include='all' includes non-numeric. info() shows: number of rows, column names, non-null counts, dtypes, memory usage. Use describe() for data distribution, info() for structure and missing data overview. Both are essential for initial data exploration.",
        "difficulty": "Medium",
        "time_estimate": 75
      }
    ],
    "NumPy": [
      {
        "question": "What is the difference between np.array([1, 2, 3]) and np.array([[1, 2, 3]])?",
        "options": [
          "They are identical",
          "First is 1D (shape (3,)); second is 2D (shape (1, 3))",
          "First is faster",
          "Second stores more data"
        ],
        "correct_answer": 1,
        "explanation": "Shape matters! [1,2,3] creates 1D array with shape (3,). [[1,2,3]] creates 2D array with shape (1, 3) - one row, three columns. This affects operations: 1D arrays don't have explicit row/column orientation, while 2D do. For matrix operations, 2D is often needed. Check with arr.shape. Use arr.reshape() to convert between shapes.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What does NumPy broadcasting allow you to do?",
        "options": [
          "Transmit data over network",
          "Perform operations on arrays of different shapes by automatically expanding them",
          "Increase array size",
          "Parallelize computations"
        ],
        "correct_answer": 1,
        "explanation": "Broadcasting allows arithmetic operations on arrays of different shapes without explicit replication. Rules: (1) Dimensions are aligned from right, (2) Dimensions of size 1 are stretched, (3) Dimensions must match or be 1. Example: (3,1) + (4,) → both broadcast to (3,4). Enables efficient memory usage and cleaner code. Understanding broadcasting is key to vectorized NumPy operations.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the difference between np.dot(A, B) and A * B for 2D arrays?",
        "options": [
          "They are identical",
          "np.dot() performs matrix multiplication; * performs element-wise multiplication",
          "np.dot() is deprecated",
          "* performs matrix multiplication"
        ],
        "correct_answer": 1,
        "explanation": "* (or np.multiply()) is element-wise multiplication: corresponding elements are multiplied. Requires same shape. np.dot() (or @ operator in Python 3.5+) performs matrix/dot product: (m,n) @ (n,p) → (m,p). For 1D arrays, dot is inner product. For higher dimensions, see np.matmul(). Use @ for matrix multiplication, * for element-wise.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What does arr.reshape(-1, 1) do?",
        "options": [
          "Deletes the array",
          "Reshapes to column vector: infers first dimension, sets second to 1",
          "Transposes the array",
          "Flattens the array"
        ],
        "correct_answer": 1,
        "explanation": "reshape(-1, 1) creates a column vector. -1 means 'infer this dimension'. For arr with 6 elements, (-1,1) becomes (6,1). (-1,) or .flatten() creates 1D. (1,-1) creates row vector. reshape doesn't copy data (returns view) if possible. reshape(-1) is common for flattening multi-dimensional arrays to 1D. Must preserve total element count.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the purpose of np.where(condition, x, y)?",
        "options": [
          "To find array indices",
          "Returns array with elements from x where condition is True, from y where False",
          "To filter array",
          "To sort array"
        ],
        "correct_answer": 1,
        "explanation": "np.where() is vectorized if-else: where condition is True, take from x; else take from y. Example: np.where(arr > 0, arr, 0) replaces negative values with 0. With just condition, np.where(condition) returns indices where True (tuple of arrays). Powerful for conditional operations without loops. Similar to array[condition] = value for boolean indexing.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "What does np.random.seed(42) do?",
        "options": [
          "Plants random numbers",
          "Sets the random number generator seed for reproducibility",
          "Generates 42 random numbers",
          "Deletes random state"
        ],
        "correct_answer": 1,
        "explanation": "Setting seed ensures reproducible random numbers. Same seed → same sequence. Essential for debugging and reproducibility in ML experiments. Use before random operations: np.random.seed(42); np.random.randn(5) always gives same 5 numbers. Modern approach: rng = np.random.default_rng(42); rng.random() for better random generation and isolation.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "What is the difference between np.sum(arr, axis=0) and np.sum(arr, axis=1) for a 2D array?",
        "options": [
          "They are identical",
          "axis=0 sums down columns (row-wise sum); axis=1 sums across rows (column-wise sum)",
          "axis=0 is faster",
          "axis=1 is deprecated"
        ],
        "correct_answer": 1,
        "explanation": "axis specifies which dimension to collapse. For 2D array (rows, cols): axis=0 aggregates along rows (down columns), returning one value per column. axis=1 aggregates along columns (across rows), returning one value per row. Think of axis as the dimension that disappears. Same logic for mean, max, etc. No axis means aggregate all elements.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What does np.arange(0, 10, 2) create?",
        "options": [
          "Array [0, 2, 4, 6, 8]",
          "Array [0, 1, 2, ... 10]",
          "Array [2, 4, 6, 8, 10]",
          "Array [0, 10, 2]"
        ],
        "correct_answer": 0,
        "explanation": "np.arange(start, stop, step) creates array from start to stop (exclusive) with step. Here: [0, 2, 4, 6, 8]. Similar to Python's range() but returns NumPy array. For floats, prefer np.linspace(start, stop, num) which includes stop and specifies count instead of step, avoiding floating-point issues. arange is half-open [start, stop).",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "What is the purpose of np.newaxis in NumPy?",
        "options": [
          "Creates a new array",
          "Adds a new dimension of size 1 to an array",
          "Deletes an axis",
          "Transposes the array"
        ],
        "correct_answer": 1,
        "explanation": "np.newaxis (or None) increases dimensionality by adding axis of size 1. arr[np.newaxis, :] converts (n,) to (1,n). arr[:, np.newaxis] converts to (n,1). Useful for broadcasting: (3,) and (4,) can't broadcast, but (3,1) and (4,) → (3,4). Cleaner than reshape for adding dimensions. Essential for proper broadcasting in matrix operations.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "What does arr.flatten() vs arr.ravel() do?",
        "options": [
          "They are always identical",
          "flatten() always copies; ravel() returns view if possible (faster but changes affect original)",
          "ravel() is deprecated",
          "flatten() is faster"
        ],
        "correct_answer": 1,
        "explanation": "Both convert multi-dimensional array to 1D. ravel() returns a view when possible (contiguous memory) - modifications affect original. flatten() always returns a copy - safe to modify. ravel() is faster and memory-efficient for large arrays. reshape(-1) is like ravel(). Use flatten() when you need an independent copy, ravel() for efficiency when view is acceptable.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the purpose of np.concatenate([arr1, arr2], axis=0)?",
        "options": [
          "Multiplies arrays",
          "Joins arrays along specified axis (axis=0 means vertically/row-wise)",
          "Finds common elements",
          "Splits arrays"
        ],
        "correct_answer": 1,
        "explanation": "concatenate() joins arrays along existing axis. axis=0 stacks vertically (appends rows), axis=1 horizontally (appends columns). Arrays must have compatible shapes (all dimensions except concatenation axis must match). Alternatives: np.vstack() (vertical), np.hstack() (horizontal), np.stack() (creates new axis). concatenate is more general and flexible.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What does np.argmax(arr) return?",
        "options": [
          "The maximum value",
          "The index of the maximum value",
          "An array of all maximum values",
          "The count of maximum values"
        ],
        "correct_answer": 1,
        "explanation": "argmax() returns the index (not value) of the maximum element. For 1D: single integer. For multi-D without axis: flattened index. With axis: indices along that axis. Example: arr = [3,1,4,2]; np.argmax(arr) = 2. For value, use arr.max() or arr[np.argmax(arr)]. Similarly, argmin() for minimum index. Useful for classification (getting predicted class).",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is the purpose of np.linspace(0, 1, 5)?",
        "options": [
          "Creates array [0, 1, 2, 3, 4]",
          "Creates array [0.0, 0.25, 0.5, 0.75, 1.0] - 5 evenly spaced values from 0 to 1",
          "Creates 5 random numbers between 0 and 1",
          "Creates array [0, 1, 0, 1, 0]"
        ],
        "correct_answer": 1,
        "explanation": "linspace(start, stop, num) creates num evenly spaced values from start to stop (inclusive). Here: [0, 0.25, 0.5, 0.75, 1.0]. Unlike arange (uses step), linspace uses count. endpoint=False excludes stop. Useful for plotting, creating grids. Preferred over arange for floats to avoid precision issues. Stop is included by default.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What does arr[:, ::-1] do for a 2D array?",
        "options": [
          "Transposes the array",
          "Reverses columns (flips horizontally)",
          "Reverses rows",
          "Deletes last column"
        ],
        "correct_answer": 1,
        "explanation": "Slicing with negative step reverses. [:, ::-1] means: all rows (:), all columns reversed (::-1). This flips columns horizontally. [::-1, :] reverses rows (vertical flip). [::-1, ::-1] reverses both. These create views (no copy). Important: negative step creates reversed view efficiently without copying data.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the difference between np.copy(arr) and arr.view()?",
        "options": [
          "They are identical",
          "copy() creates independent copy; view() creates new array object sharing same data",
          "view() is faster and always preferred",
          "copy() shares data"
        ],
        "correct_answer": 1,
        "explanation": "copy() creates independent deep copy - changes don't affect original. view() creates new array object but shares underlying data - changes affect both. Slicing usually creates views. Use copy() when you need independence. view() is memory-efficient but requires care. To check: arr.base is None for copy, refers to original for view. Assignment (b=a) creates reference (not even a view).",
        "difficulty": "Hard",
        "time_estimate": 95
      }
    ],
    "OOP": [
      {
        "question": "What is the purpose of encapsulation in object-oriented programming?",
        "options": [
          "To make code run faster",
          "To bundle data and methods that operate on that data, hiding internal details",
          "To create multiple classes",
          "To enable inheritance"
        ],
        "correct_answer": 1,
        "explanation": "Encapsulation bundles data (attributes) and methods together in a class while hiding internal implementation details. This is achieved through access modifiers (private, protected, public) and provides controlled access via getters/setters. Benefits: (1) data protection, (2) reduced coupling, (3) easier maintenance. Example: making attributes private and providing public methods to access them.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "In Python, what is the difference between class variables and instance variables?",
        "options": [
          "They are identical",
          "Class variables are shared by all instances; instance variables are unique to each instance",
          "Class variables are faster",
          "Instance variables can't be modified"
        ],
        "correct_answer": 1,
        "explanation": "Class variables are defined in the class body and shared by all instances: changes affect all. Instance variables are defined in __init__ with self.var and unique to each instance. Access: ClassName.class_var for class variables, instance.var for instance variables. Class variables are useful for constants or counters shared across instances. Instance variables store object-specific state.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is polymorphism in OOP?",
        "options": [
          "Creating multiple classes",
          "The ability of objects of different classes to respond to the same method call in different ways",
          "Making classes private",
          "Inheriting from multiple parents"
        ],
        "correct_answer": 1,
        "explanation": "Polymorphism allows different classes to implement the same interface differently. Two types: (1) Compile-time (method overloading), (2) Runtime (method overriding via inheritance). Example: Shape classes (Circle, Square) each implement draw() differently. Enables writing generic code that works with any Shape. Duck typing in Python: 'if it walks like a duck and quacks like a duck, it's a duck' - focus on methods, not types.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the purpose of the __init__ method in Python classes?",
        "options": [
          "To delete the object",
          "Constructor method that initializes object state when an instance is created",
          "To print the object",
          "To make the class abstract"
        ],
        "correct_answer": 1,
        "explanation": "__init__ is the constructor called automatically when creating an instance. It initializes instance variables. Syntax: def __init__(self, params): self.attribute = value. self refers to the instance being created. __init__ returns None (shouldn't return anything). For cleanup, use __del__ (destructor). __new__ is the actual object creator (rarely overridden).",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "What is the difference between inheritance and composition?",
        "options": [
          "They are the same",
          "Inheritance is 'is-a' relationship (subclass extends parent); composition is 'has-a' relationship (object contains other objects)",
          "Inheritance is always better",
          "Composition is deprecated"
        ],
        "correct_answer": 1,
        "explanation": "Inheritance: class Dog(Animal) - Dog IS-A Animal, inherits Animal's methods. Composition: class Car contains Engine - Car HAS-AN Engine. Composition is often preferred (favor composition over inheritance) as it's more flexible, reduces tight coupling, and avoids deep inheritance hierarchies. Use inheritance for true 'is-a' relationships, composition for 'has-a' or 'uses-a'. Multiple composition vs. multiple inheritance issues.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is method overriding in OOP?",
        "options": [
          "Creating multiple methods with same name in one class",
          "Subclass providing specific implementation of method already defined in parent class",
          "Deleting parent methods",
          "Making methods private"
        ],
        "correct_answer": 1,
        "explanation": "Method overriding allows a subclass to provide specific implementation of a method inherited from parent. The overridden method in subclass has same name and signature. When called on subclass instance, subclass version executes. Use super().method() to call parent version. Enables polymorphism - different behavior based on actual object type at runtime. Different from overloading (same name, different parameters).",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is an abstract class?",
        "options": [
          "A class that is difficult to understand",
          "A class that cannot be instantiated and serves as a base template for subclasses",
          "A class with no methods",
          "A class that is deprecated"
        ],
        "correct_answer": 1,
        "explanation": "Abstract classes define interfaces that subclasses must implement. Cannot be instantiated directly. In Python, use ABC module: class MyClass(ABC): @abstractmethod def my_method(): pass. Subclasses must implement all abstract methods. Use abstract classes to enforce a contract - ensuring all subclasses have required methods. Different from interfaces (pure abstract classes with no implementation).",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What does the @property decorator do in Python?",
        "options": [
          "Makes a method into a property with getter/setter behavior",
          "Deletes the method",
          "Makes the method static",
          "Speeds up execution"
        ],
        "correct_answer": 0,
        "explanation": "@property converts a method into a getter, allowing attribute-like access. @property def x(self): return self._x allows obj.x instead of obj.x(). Provide setter with @x.setter def x(self, value): self._x = value. This enables encapsulation - control access to attributes while maintaining clean syntax. Can add validation in setters. Use _variable convention for internal attributes.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the purpose of __str__ and __repr__ methods in Python?",
        "options": [
          "They delete objects",
          "__str__ for human-readable string (print); __repr__ for unambiguous representation (debugging)",
          "They are identical",
          "__str__ is deprecated"
        ],
        "correct_answer": 1,
        "explanation": "__str__ should return a user-friendly string (used by str() and print()). __repr__ should return an unambiguous string ideally usable to recreate the object (used by repr() and in interactive shell). Best practice: __repr__ for developers, __str__ for end users. If only one, define __repr__ (used as fallback for __str__). Example: __repr__ = 'Point(1, 2)', __str__ = '(1, 2)'.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is multiple inheritance and what problem does it create?",
        "options": [
          "Inheriting from one class",
          "Inheriting from multiple parent classes; creates diamond problem when parents share a common ancestor",
          "Creating multiple objects",
          "A deprecated feature"
        ],
        "correct_answer": 1,
        "explanation": "Multiple inheritance: class C(A, B) inherits from both A and B. Diamond problem: if A and B inherit from Base, which Base version does C use? Python solves this with MRO (Method Resolution Order) using C3 linearization. Check with Class.__mro__ or Class.mro(). super() follows MRO. While powerful, multiple inheritance can be complex - prefer composition or mixins. Use for mixins (adding functionality).",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is the difference between @staticmethod and @classmethod in Python?",
        "options": [
          "They are identical",
          "staticmethod doesn't receive implicit first argument; classmethod receives class as first argument (cls)",
          "staticmethod is deprecated",
          "classmethod is faster"
        ],
        "correct_answer": 1,
        "explanation": "@staticmethod: no implicit first argument, can't access instance or class. Use for utility functions related to the class. @classmethod: receives class as first argument (cls), can access/modify class state. Use for factory methods. Instance methods receive instance (self). Examples: @staticmethod def utility(x): ...; @classmethod def from_string(cls, s): return cls(...) creates instance from string.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the Liskov Substitution Principle (LSP)?",
        "options": [
          "Lists should be substituted with arrays",
          "Objects of a subclass should be replaceable with objects of the superclass without breaking the application",
          "A sorting algorithm",
          "A Python built-in function"
        ],
        "correct_answer": 1,
        "explanation": "LSP states: if S is a subtype of T, objects of type T can be replaced with objects of type S without altering program correctness. Subclasses must honor the contract of parent class. Violations: subclass throwing new exceptions, strengthening preconditions, weakening postconditions. Ensures inheritance is used correctly. Example: if Square inherits Rectangle but can't set width independently, it violates LSP.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the Single Responsibility Principle (SRP)?",
        "options": [
          "A class should have only one method",
          "A class should have only one reason to change - one responsibility",
          "A class should inherit from one parent only",
          "A class should have one instance"
        ],
        "correct_answer": 1,
        "explanation": "SRP (from SOLID): a class should have one responsibility - one reason to change. This improves maintainability and reduces coupling. Bad: UserClass handling authentication, database, and email. Good: separate UserAuth, UserRepository, EmailService classes. Each class does one thing well. Makes code easier to test, understand, and modify. Applies to functions too - single, well-defined purpose.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is dependency injection?",
        "options": [
          "Deleting dependencies",
          "Providing dependencies to a class from outside rather than creating them internally",
          "A type of inheritance",
          "A Python library"
        ],
        "correct_answer": 1,
        "explanation": "Dependency Injection: pass dependencies to a class rather than creating them inside. Instead of class A: def __init__(self): self.b = B(), use class A: def __init__(self, b): self.b = b. Benefits: (1) loose coupling, (2) easier testing (inject mocks), (3) flexibility (swap implementations). DI container frameworks automate this. Related to Dependency Inversion Principle (depend on abstractions, not concretions).",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the purpose of interfaces in OOP (or abstract base classes in Python)?",
        "options": [
          "To slow down code",
          "To define a contract that implementing classes must follow, ensuring consistent APIs",
          "To create GUI interfaces",
          "To connect to the internet"
        ],
        "correct_answer": 1,
        "explanation": "Interfaces define method signatures that implementing classes must provide - a contract. In Python, use ABC and @abstractmethod. Benefits: (1) enforces consistent API, (2) enables polymorphism, (3) documents expected behavior. Example: PaymentProcessor interface ensures all payment implementations have process_payment(). Use when multiple classes should share the same interface but have different implementations.",
        "difficulty": "Medium",
        "time_estimate": 85
      }
    ],
    "Algorithms": [
      {
        "question": "What is the time complexity of binary search?",
        "options": [
          "O(n)",
          "O(log n)",
          "O(n log n)",
          "O(1)"
        ],
        "correct_answer": 1,
        "explanation": "Binary search has O(log n) time complexity. It divides the search space in half each iteration, working only on sorted arrays. After k iterations, search space is n/2^k. When n/2^k = 1, k = log₂(n). Space: O(1) iterative, O(log n) recursive (call stack). Much faster than linear search O(n) for large datasets. Prerequisite: sorted array.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "What is the difference between BFS (Breadth-First Search) and DFS (Depth-First Search)?",
        "options": [
          "They are identical",
          "BFS explores level by level using a queue; DFS explores as deep as possible using a stack",
          "BFS is always faster",
          "DFS can't find paths"
        ],
        "correct_answer": 1,
        "explanation": "BFS uses queue (FIFO): explores all neighbors before going deeper, finds shortest path in unweighted graphs. DFS uses stack (LIFO) or recursion: explores as far as possible before backtracking. BFS better for shortest path, DFS for existence checks/topological sort. Space: BFS O(width), DFS O(height). Time: both O(V+E) for graphs. Use BFS for shortest path, DFS for cycle detection, topological sort.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the time complexity of quicksort in the average case?",
        "options": [
          "O(n)",
          "O(n log n)",
          "O(n²)",
          "O(log n)"
        ],
        "correct_answer": 1,
        "explanation": "Quicksort average case: O(n log n). Worst case: O(n²) when pivot is always smallest/largest (e.g., already sorted). Best case: O(n log n). Space: O(log n) for recursion stack. Randomized pivot selection avoids worst case in practice. In-place sorting (unlike merge sort). Despite worst case, often faster than merge sort due to better cache performance and in-place operation.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What data structure should you use to implement a LRU (Least Recently Used) cache efficiently?",
        "options": [
          "Array only",
          "Hash map + doubly linked list",
          "Binary tree",
          "Stack"
        ],
        "correct_answer": 1,
        "explanation": "LRU cache requires O(1) get and O(1) put. Solution: Hash map for O(1) access + doubly linked list for O(1) reordering. Hash map stores key→node, linked list maintains access order (most recent at head). On access: move node to head. On capacity: remove tail. Hash map alone can't track order efficiently, linked list alone can't find keys quickly. OrderedDict in Python implements this pattern.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is dynamic programming?",
        "options": [
          "Programming that changes at runtime",
          "Solving problems by breaking them into overlapping subproblems and storing results to avoid recomputation",
          "A programming language",
          "Parallel programming"
        ],
        "correct_answer": 1,
        "explanation": "Dynamic Programming (DP) solves optimization problems by: (1) breaking into overlapping subproblems, (2) storing results (memoization or tabulation) to avoid recomputation. Key: optimal substructure + overlapping subproblems. Approaches: top-down (memoization/recursion), bottom-up (tabulation/iteration). Classic examples: Fibonacci, knapsack, longest common subsequence. Transforms exponential problems to polynomial. Different from divide-and-conquer (non-overlapping subproblems).",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the time complexity of inserting an element at the beginning of a linked list vs. an array?",
        "options": [
          "Both O(1)",
          "Linked list O(1), array O(n)",
          "Linked list O(n), array O(1)",
          "Both O(n)"
        ],
        "correct_answer": 1,
        "explanation": "Linked list: O(1) - create new node, point to current head, update head. Array: O(n) - shift all elements right to make space at index 0. This is why linked lists excel at insertions/deletions at ends, while arrays provide O(1) random access. Dynamic arrays (Python list) amortize append to O(1) but prepend remains O(n). For frequent prepends, use deque (doubly-linked list with O(1) operations at both ends).",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is a hash collision and how can it be resolved?",
        "options": [
          "When two files have the same hash",
          "When two keys map to the same hash table index; resolved via chaining or open addressing",
          "A type of car accident",
          "An error in the algorithm"
        ],
        "correct_answer": 1,
        "explanation": "Hash collision occurs when hash function maps different keys to the same index. Resolution methods: (1) Chaining - each bucket contains a linked list of colliding entries, (2) Open addressing - probe for next available slot (linear, quadratic, double hashing). Chaining allows more elements than slots, open addressing requires good probing. Python dict uses open addressing. Good hash function minimizes collisions.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the time complexity of heapsort?",
        "options": [
          "O(n)",
          "O(n log n)",
          "O(n²)",
          "O(log n)"
        ],
        "correct_answer": 1,
        "explanation": "Heapsort: O(n log n) in all cases (best, average, worst). Build heap: O(n). Extract max n times, each O(log n) for heapify: n * log n. Space: O(1) as it's in-place. Advantages: guaranteed O(n log n), in-place. Disadvantages: not stable, worse cache performance than quicksort. Heap is also used for priority queue. Python heapq provides min-heap.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is memoization?",
        "options": [
          "Remembering things manually",
          "Caching function results based on arguments to avoid recomputation",
          "A memory management technique",
          "Deleting old data"
        ],
        "correct_answer": 1,
        "explanation": "Memoization caches function results keyed by arguments. On subsequent calls with same arguments, return cached result. Enables top-down DP. Example: Fibonacci with dict to store computed values. Python: use @lru_cache decorator. Trade-off: memory for speed. Only for pure functions (same inputs → same output). Different from tabulation (bottom-up DP). functools.lru_cache limits cache size.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the difference between a min-heap and a max-heap?",
        "options": [
          "They are identical",
          "Min-heap: parent ≤ children (root is minimum); max-heap: parent ≥ children (root is maximum)",
          "Min-heap is smaller in size",
          "Max-heap uses more memory"
        ],
        "correct_answer": 1,
        "explanation": "Min-heap: parent node value ≤ children, root is minimum. Max-heap: parent ≥ children, root is maximum. Both complete binary trees (all levels full except possibly last, filled left-to-right). Operations: insert O(log n), extract min/max O(log n), peek O(1). Python heapq is min-heap (negate values for max-heap). Use min-heap for smallest element priority, max-heap for largest.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is a trie (prefix tree) used for?",
        "options": [
          "Sorting numbers",
          "Efficient storage and retrieval of strings, especially for prefix searches",
          "Binary search",
          "Hashing"
        ],
        "correct_answer": 1,
        "explanation": "Trie stores strings efficiently by sharing common prefixes. Each node represents a character, paths form words. Operations: insert/search/delete O(m) where m = string length. Space: O(ALPHABET_SIZE * m * n) for n strings. Use cases: autocomplete, spell checker, IP routing. Advantages over hash table: prefix queries, sorted order. Disadvantage: space overhead. Compressed variant: radix tree.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the time complexity of merging two sorted arrays of size m and n?",
        "options": [
          "O(m + n)",
          "O(m * n)",
          "O(log(m + n))",
          "O(max(m, n))"
        ],
        "correct_answer": 0,
        "explanation": "Merging two sorted arrays: O(m + n) time, O(m + n) space for result. Algorithm: use two pointers, compare elements, take smaller, advance pointer. Must traverse all elements of both arrays exactly once. Cannot be faster than O(m+n) as you must examine all elements. This is the merge step in merge sort. In-place merging (given extra space in one array) is more complex but achievable.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is a balanced binary search tree and why is it important?",
        "options": [
          "A tree with equal left and right subtrees",
          "A BST where height is O(log n), ensuring operations remain efficient",
          "A tree with all leaves at same level",
          "A sorting algorithm"
        ],
        "correct_answer": 1,
        "explanation": "Balanced BST maintains height O(log n) for n nodes, ensuring search/insert/delete remain O(log n). Unbalanced BST can degrade to O(n) in worst case (e.g., inserting sorted data). Self-balancing implementations: AVL (strict balance), Red-Black (relaxed balance, faster updates), B-trees (for disk). Python: no built-in balanced BST (use bisect + list or sortedcontainers library). Crucial for databases and file systems.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the space complexity of merge sort?",
        "options": [
          "O(1)",
          "O(n)",
          "O(log n)",
          "O(n log n)"
        ],
        "correct_answer": 1,
        "explanation": "Merge sort space: O(n) for auxiliary array used in merging. Time: O(n log n) all cases. Stable sort (preserves relative order of equal elements). Not in-place (unlike quicksort). Recursion depth: O(log n) call stack. Total space: O(n) + O(log n) = O(n). Good for linked lists (O(1) space), external sorting (large datasets on disk). Parallelizable.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is topological sort and when is it used?",
        "options": [
          "Sorting numbers",
          "Linear ordering of directed acyclic graph (DAG) vertices respecting edge directions",
          "Sorting alphabetically",
          "A hash function"
        ],
        "correct_answer": 1,
        "explanation": "Topological sort orders DAG vertices so for every edge u→v, u comes before v. Use cases: task scheduling with dependencies, build systems, course prerequisites. Algorithms: (1) DFS with stack, (2) Kahn's (BFS with in-degree). Only possible for DAGs (cyclic graphs have no topological order). Multiple valid orderings possible. Time: O(V+E). Detects cycles if unable to order all vertices.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the difference between comparison-based and non-comparison-based sorting?",
        "options": [
          "No difference",
          "Comparison-based uses comparisons (e.g., quicksort), lower bound O(n log n); non-comparison uses counting/radix, can be O(n)",
          "Comparison-based is always faster",
          "Non-comparison doesn't work"
        ],
        "correct_answer": 1,
        "explanation": "Comparison-based (quicksort, mergesort, heapsort): O(n log n) lower bound proven. Non-comparison (counting sort, radix sort, bucket sort): can achieve O(n) by exploiting properties of input (e.g., limited range, digit-by-digit). Counting sort: O(n+k) for range k. Radix: O(d*n) for d digits. Bucket: O(n) average for uniformly distributed data. Trade-off: non-comparison needs assumptions about input.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "What is a greedy algorithm?",
        "options": [
          "An algorithm that uses a lot of memory",
          "An algorithm that makes locally optimal choices at each step, hoping to find global optimum",
          "An algorithm that always fails",
          "An algorithm that runs slowly"
        ],
        "correct_answer": 1,
        "explanation": "Greedy algorithms make locally optimal choice at each step, hoping for global optimum. Doesn't always work (needs greedy-choice property and optimal substructure). Examples that work: Dijkstra's shortest path, Huffman coding, Kruskal's MST. Counter-example: greedy fails for making change with arbitrary coin denominations. Faster than DP but less general. Prove correctness: show local choices lead to global optimum.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "What is the master theorem used for?",
        "options": [
          "Solving linear equations",
          "Analyzing time complexity of divide-and-conquer recursive algorithms",
          "Finding shortest paths",
          "Sorting algorithms"
        ],
        "correct_answer": 1,
        "explanation": "Master theorem analyzes recurrences of form T(n) = aT(n/b) + f(n) where a≥1, b>1. Compares f(n) to n^(log_b(a)). Three cases determine if dominated by: (1) leaves, (2) all levels equally, (3) root. Applies to binary search, merge sort, many divide-and-conquer algorithms. Doesn't handle all recurrences (e.g., T(n) = T(n-1) + n). Powerful tool for algorithmic analysis.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the Floyd-Warshall algorithm used for?",
        "options": [
          "Sorting arrays",
          "Finding shortest paths between all pairs of vertices in a weighted graph",
          "Binary search",
          "String matching"
        ],
        "correct_answer": 1,
        "explanation": "Floyd-Warshall finds shortest paths between all vertex pairs in weighted graph (positive or negative edges, but no negative cycles). Time: O(V³), Space: O(V²). DP algorithm: for each intermediate vertex k, check if path through k is shorter. Simpler than running Dijkstra V times. Detects negative cycles. Returns distance matrix. Use when you need all-pairs shortest paths.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the difference between divide-and-conquer and dynamic programming?",
        "options": [
          "They are identical",
          "D&C has non-overlapping subproblems; DP has overlapping subproblems (stores results to avoid recomputation)",
          "D&C is always slower",
          "DP doesn't use recursion"
        ],
        "correct_answer": 1,
        "explanation": "Both break problems into subproblems. Divide-and-conquer: subproblems are independent (e.g., merge sort splits array, subproblems don't overlap). DP: subproblems overlap (e.g., Fibonacci: fib(n-1) and fib(n-2) both compute fib(n-3)), so cache results. DP requires optimal substructure + overlapping subproblems. D&C solves each subproblem once. DP avoids recomputation through memoization/tabulation.",
        "difficulty": "Hard",
        "time_estimate": 100
      }
    ],
    "REST APIs": [
      {
        "question": "What does REST stand for and what is its key principle?",
        "options": [
          "Random Execution State Transfer",
          "Representational State Transfer - stateless client-server communication",
          "Rapid Execution System Transfer",
          "Remote Execution Service Transfer"
        ],
        "correct_answer": 1,
        "explanation": "REST (Representational State Transfer) is an architectural style for distributed systems. Key principles: (1) Stateless - each request contains all necessary information, (2) Client-Server separation, (3) Cacheable responses, (4) Uniform interface, (5) Layered system. REST APIs use HTTP methods (GET, POST, PUT, DELETE) on resources identified by URIs. Stateless means no client context stored on server between requests.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the difference between PUT and PATCH HTTP methods?",
        "options": [
          "They are identical",
          "PUT replaces entire resource; PATCH partially updates resource",
          "PUT is for creation only",
          "PATCH is deprecated"
        ],
        "correct_answer": 1,
        "explanation": "PUT is idempotent and replaces the entire resource - send complete representation. PATCH is for partial updates - send only changed fields. Example: PUT /users/1 with {name, email, age} replaces all fields. PATCH /users/1 with {email} updates only email. PUT idempotent: multiple identical requests have same effect. PATCH may not be idempotent depending on implementation. Use PUT for full updates, PATCH for partial.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "What does a 401 Unauthorized HTTP status code indicate?",
        "options": [
          "Server error",
          "Authentication required or failed - client must authenticate",
          "Resource not found",
          "Request succeeded"
        ],
        "correct_answer": 1,
        "explanation": "401 Unauthorized means authentication is required or has failed. Client must provide valid credentials. Actually should be called 'Unauthenticated'. Different from 403 Forbidden (authenticated but lacks permission). Response should include WWW-Authenticate header indicating authentication method. Common in APIs requiring API keys, OAuth tokens, or basic auth. Fix: provide valid credentials in Authorization header.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is the purpose of HTTP status code 204 No Content?",
        "options": [
          "Error occurred",
          "Request succeeded but no content to return (common for DELETE)",
          "Content not found",
          "Partial content"
        ],
        "correct_answer": 1,
        "explanation": "204 No Content indicates successful request with no response body. Common for: (1) DELETE operations (successfully deleted, nothing to return), (2) PUT/PATCH where response body isn't needed. Status 200 would include response body. Saves bandwidth when response body isn't necessary. Different from 404 (not found) or 201 (created with resource in body). Client shouldn't expect body with 204.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "What is idempotency in REST APIs?",
        "options": [
          "Making requests faster",
          "Making the same request multiple times produces the same result",
          "Encrypting requests",
          "Compressing responses"
        ],
        "correct_answer": 1,
        "explanation": "Idempotent operations produce the same result regardless of how many times they're executed. Idempotent HTTP methods: GET, PUT, DELETE, HEAD, OPTIONS. Non-idempotent: POST. Example: DELETE /user/1 multiple times - first deletes, rest do nothing (resource stays deleted). PUT /user/1 {data} multiple times - resource stays in same state. Important for retries and reliability. POST creates new resource each time (not idempotent).",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "What is the purpose of the Accept header in HTTP requests?",
        "options": [
          "To accept cookies",
          "To specify what media types (content types) the client can handle in response",
          "To authorize the request",
          "To compress data"
        ],
        "correct_answer": 1,
        "explanation": "Accept header specifies media types client can process in response. Example: Accept: application/json requests JSON format. Accept: application/xml for XML. Accept: */* accepts any format. Server uses content negotiation to return appropriate format or 406 Not Acceptable if it can't provide requested format. Related: Content-Type (what you're sending), Accept-Language (preferred language).",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is CORS (Cross-Origin Resource Sharing)?",
        "options": [
          "A database technology",
          "A mechanism that allows restricted resources to be requested from another domain",
          "A programming language",
          "A sorting algorithm"
        ],
        "correct_answer": 1,
        "explanation": "CORS allows controlled access to resources from different origins (domain, protocol, or port). Browser security policy normally blocks cross-origin requests. Server includes CORS headers (Access-Control-Allow-Origin, etc.) to permit. Preflight requests (OPTIONS) check permissions before actual request. Common issue: API doesn't include CORS headers, browser blocks request. Configured server-side. Essential for web apps calling external APIs.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "What is the difference between authentication and authorization?",
        "options": [
          "They are the same",
          "Authentication verifies identity (who you are); authorization verifies permissions (what you can do)",
          "Authorization comes first",
          "Authentication is deprecated"
        ],
        "correct_answer": 1,
        "explanation": "Authentication (AuthN): verifying identity - 'Who are you?' - login with username/password, API key, OAuth. Authorization (AuthZ): verifying permissions - 'What can you do?' - checking if authenticated user can access resource. Flow: authenticate first, then authorize. HTTP: 401 for authentication failure, 403 for authorization failure. Example: login (authenticate), then check if user can delete post (authorize).",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is rate limiting in APIs and why is it important?",
        "options": [
          "Making APIs slower",
          "Restricting number of requests a client can make in a time period to prevent abuse",
          "Speeding up responses",
          "Compressing data"
        ],
        "correct_answer": 1,
        "explanation": "Rate limiting restricts requests per time window (e.g., 100/minute, 1000/hour) to: (1) prevent abuse/DoS, (2) ensure fair usage, (3) control costs. Common headers: X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset. Status 429 Too Many Requests when exceeded. Algorithms: token bucket, leaky bucket, fixed/sliding window. Important for public APIs. Implement per-user or per-IP.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "What is the purpose of API versioning?",
        "options": [
          "To make APIs slower",
          "To manage changes and maintain backward compatibility while evolving API",
          "To delete old endpoints",
          "To compress responses"
        ],
        "correct_answer": 1,
        "explanation": "API versioning allows introducing breaking changes without disrupting existing clients. Strategies: (1) URI versioning (/v1/users, /v2/users), (2) Header versioning (Accept: application/vnd.api+json;version=1), (3) Query parameter (?version=1). URI versioning most common. Allows: deprecating old versions gradually, supporting multiple versions simultaneously. Important for public APIs with many clients. Minimize versions - maintain only necessary ones.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "What is the difference between HTTP methods GET and POST?",
        "options": [
          "They are identical",
          "GET retrieves data, is idempotent, cached; POST creates/submits data, not idempotent, not cached",
          "GET is deprecated",
          "POST is for deletion"
        ],
        "correct_answer": 1,
        "explanation": "GET: retrieves resources, idempotent, cacheable, params in URL (query string), should not modify server state. POST: creates resources or submits data, not idempotent, not cacheable, params in body, modifies server state. GET limited by URL length, POST not limited. GET bookmarkable/linkable. Use GET for reads, POST for writes/creates. Security: don't send sensitive data in GET (URLs logged). GET requests shouldn't have side effects.",
        "difficulty": "Medium",
        "time_estimate": 75
      },
      {
        "question": "What is a RESTful resource and how should it be named?",
        "options": [
          "A random endpoint",
          "An entity or concept in the system, named with nouns (not verbs) in plural form",
          "A function name",
          "A database table"
        ],
        "correct_answer": 1,
        "explanation": "Resources are entities/concepts (users, products, orders). Naming: (1) use nouns, not verbs (GET /users, not /getUsers), (2) plural form (/users), (3) hierarchical (/users/123/orders), (4) lowercase with hyphens (/order-items). HTTP methods provide the verbs (GET, POST, PUT, DELETE). Good: GET /users/123, POST /users. Bad: GET /getUser?id=123, POST /createUser. Resources should represent domain concepts clearly.",
        "difficulty": "Medium",
        "time_estimate": 80
      }
    ],
    "Problem Solving": [
      {
        "question": "A data pipeline is failing intermittently. What is the BEST first step to diagnose the issue?",
        "options": [
          "Immediately rewrite the entire pipeline",
          "Gather logs and identify patterns in when failures occur",
          "Restart the server repeatedly",
          "Assume it's a hardware issue"
        ],
        "correct_answer": 1,
        "explanation": "Systematic problem-solving starts with data collection and pattern analysis. Logs reveal: failure frequency, error messages, resource usage, timing patterns. This evidence guides hypothesis formation. Common patterns: time-based (scheduled jobs), data-based (specific inputs), resource-based (memory/CPU). Premature solutions (rewriting code) waste time. The scientific method applies: observe, hypothesize, test.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "Your ML model performs well in development but poorly in production. What is the MOST likely cause?",
        "options": [
          "The model architecture is wrong",
          "Train-test distribution mismatch (data drift)",
          "The programming language is wrong",
          "Too much training data"
        ],
        "correct_answer": 1,
        "explanation": "Production data often differs from training data (concept drift, data drift, covariate shift). Causes: time-based changes, different user populations, data collection biases. Solutions: (1) monitor data distributions, (2) retrain regularly, (3) use validation data closer to production, (4) A/B test carefully. Always validate on realistic data. This is more common than architectural issues when dev performance is good.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "You need to optimize a slow SQL query. What should you check FIRST?",
        "options": [
          "Rewrite in a different language",
          "Check if indexes exist on columns used in WHERE, JOIN, and ORDER BY clauses",
          "Buy faster hardware",
          "Remove all data"
        ],
        "correct_answer": 1,
        "explanation": "Missing indexes are the most common cause of slow queries. Indexes enable fast lookups (B-tree traversal O(log n) vs table scan O(n)). Check: (1) WHERE clause columns, (2) JOIN columns, (3) ORDER BY columns. Use EXPLAIN/EXPLAIN ANALYZE to see query plan. Other issues: SELECT *, unnecessary JOINs, N+1 queries. But indexes give biggest wins with least effort. Beware: too many indexes slow writes.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "A user reports a bug you cannot reproduce. What is the BEST approach?",
        "options": [
          "Tell them the bug doesn't exist",
          "Gather detailed information: environment, steps to reproduce, error messages, screenshots",
          "Close the ticket immediately",
          "Guess randomly"
        ],
        "correct_answer": 1,
        "explanation": "Unreproducible bugs require systematic information gathering: (1) exact steps taken, (2) environment (OS, browser, version), (3) error messages/logs, (4) screenshots/video, (5) data state. Often bugs are environment-specific (browser compatibility), timing-dependent (race conditions), or data-dependent (edge cases). Create a questionnaire to gather this systematically. Never assume bug doesn't exist - 'works on my machine' is a red flag.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "Your API response time degraded from 100ms to 2000ms after a deployment. How do you identify the cause?",
        "options": [
          "Roll back immediately without investigation",
          "Profile the new code, check database query times, examine external API calls, review recent changes",
          "Blame the database team",
          "Wait for it to fix itself"
        ],
        "correct_answer": 1,
        "explanation": "Performance regression requires profiling: (1) application profiler (identify slow functions), (2) database query analyzer (slow queries), (3) APM tools (external calls), (4) code diff review. Common culprits: N+1 queries, inefficient algorithms, missing caching, external API timeouts. Measure, don't guess. Tools: cProfile (Python), flame graphs, database slow query logs. Compare before/after metrics. Rollback may be necessary, but understanding prevents recurrence.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "You're asked to reduce cloud costs by 30%. What's the MOST effective approach?",
        "options": [
          "Delete all data",
          "Analyze resource utilization metrics, identify over-provisioned resources, and implement autoscaling",
          "Switch cloud providers randomly",
          "Shut down everything"
        ],
        "correct_answer": 1,
        "explanation": "Data-driven cost optimization: (1) identify largest cost centers (analytics dashboard), (2) find waste (idle resources, over-provisioning), (3) right-size instances, (4) use reserved/spot instances, (5) implement autoscaling, (6) archive old data to cheaper storage. Common wins: unused dev environments, over-provisioned databases, lack of autoscaling. 80/20 rule: 20% of resources likely account for 80% of costs.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "Two features conflict in requirements. Feature A needs low latency, Feature B needs high throughput. How do you proceed?",
        "options": [
          "Choose randomly",
          "Understand business priorities, measure trade-offs quantitatively, propose compromise or separate optimized paths",
          "Implement neither",
          "Argue with stakeholders"
        ],
        "correct_answer": 1,
        "explanation": "Conflicting requirements need: (1) stakeholder alignment on priorities (which business need is more critical?), (2) quantify trade-offs (latency vs throughput numbers), (3) explore solutions (can we have separate endpoints? different tiers?), (4) propose data-driven recommendation. Often false dichotomy - creative solutions exist. Example: async processing for throughput, synchronous for latency. Document decision rationale.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "Your team's velocity has dropped 50%. What should you investigate?",
        "options": [
          "Fire everyone",
          "Check for blockers, technical debt, context switching, unclear requirements, team morale",
          "Ignore the metrics",
          "Work longer hours only"
        ],
        "correct_answer": 1,
        "explanation": "Velocity drops indicate systemic issues: (1) technical debt slowing development, (2) unclear/changing requirements causing rework, (3) blockers (waiting for reviews, deployments, dependencies), (4) context switching (too many projects), (5) team issues (morale, turnover). Talk to the team - they know the blockers. Measure: time in code review, deployment frequency, rework percentage. Address root causes, not symptoms. Working longer hours treats symptom, not cause.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "You discover a security vulnerability in production code. What's the correct sequence of actions?",
        "options": [
          "Post about it on social media",
          "Assess severity, develop fix, deploy patch, review how it was introduced, prevent recurrence",
          "Do nothing",
          "Blame the intern"
        ],
        "correct_answer": 1,
        "explanation": "Security incident response: (1) assess severity (data exposure? active exploitation?), (2) contain (if needed, take system offline), (3) develop and test fix, (4) deploy urgently, (5) post-mortem (how introduced? why not caught?), (6) prevent recurrence (automated security scanning, training). Document timeline. Notify affected users if data compromised. Learn, don't blame. Security is everyone's responsibility.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "You're assigned a vague requirement: 'Make the system faster.' What do you do?",
        "options": [
          "Start coding randomly",
          "Define metrics, measure current performance, identify bottlenecks, set specific targets with stakeholders",
          "Quit",
          "Say it's already fast enough"
        ],
        "correct_answer": 1,
        "explanation": "Vague requirements need clarification: (1) What is 'faster'? (latency? throughput? page load?), (2) Measure current state (baseline), (3) Set specific targets (e.g., 'reduce p95 latency from 500ms to 200ms'), (4) Identify bottlenecks (profile), (5) Get stakeholder agreement on priorities. Without metrics, cannot measure success. Always define done. Specific, measurable goals enable focused optimization and validation.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "Your test suite takes 2 hours to run. How do you improve this?",
        "options": [
          "Delete all tests",
          "Parallelize tests, identify slow tests, use test categorization (unit/integration), optimize fixtures",
          "Never run tests again",
          "Buy faster computer only"
        ],
        "correct_answer": 1,
        "explanation": "Slow test suites hurt productivity. Solutions: (1) parallelize (run tests concurrently), (2) profile tests (find slow ones), (3) categorize (unit tests fast/always, integration slower/less frequent), (4) optimize setup/teardown, (5) mock external dependencies, (6) selective running (test only affected code). Goal: <10 min for most runs, comprehensive suite nightly. Fast feedback loop crucial for TDD and CI/CD.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "A stakeholder wants a feature that will take 3 months but provides minimal value. How do you respond?",
        "options": [
          "Immediately start building",
          "Understand the underlying need, propose simpler alternatives, discuss cost-benefit with data",
          "Refuse without explanation",
          "Build it poorly intentionally"
        ],
        "correct_answer": 1,
        "explanation": "Challenge requirements productively: (1) understand the why (underlying business need), (2) propose alternatives (simpler solutions to same need?), (3) quantify cost (3 months = opportunity cost of other features), (4) quantify value (how many users? revenue impact?), (5) collaborative decision with data. Sometimes seemingly simple requests mask complex needs. Other times, 80% of value achievable with 20% effort. Build trust through questioning, not defiance.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "You inherit legacy code with no documentation. What's the BEST way to understand it?",
        "options": [
          "Delete it and start over",
          "Read code systematically, trace execution with debugger, write tests to verify behavior, document as you learn",
          "Never touch it",
          "Complain constantly"
        ],
        "correct_answer": 1,
        "explanation": "Understanding legacy code: (1) start with high-level architecture (what are main components?), (2) trace key workflows with debugger, (3) write characterization tests (document current behavior), (4) identify patterns and idioms, (5) document as you learn. Don't rewrite unless necessary - rewrites rarely succeed and lose institutional knowledge. Tests provide safety net for changes. Refactor incrementally with test coverage.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "Your monitoring shows disk usage at 95%. What should you do FIRST?",
        "options": [
          "Delete random files",
          "Identify what's consuming space (logs? temp files? data growth?), then take appropriate action",
          "Ignore it until 100%",
          "Buy more servers"
        ],
        "correct_answer": 1,
        "explanation": "Disk space crisis requires quick assessment: (1) identify space consumers (du, df commands), (2) check for unexpected growth (logs exploding? temp files not cleaned?), (3) immediate relief (compress/delete old logs), (4) long-term solution (log rotation, data archival, storage expansion). Common culprits: unrotated logs, temp files, abandoned data. Set up alerts at 80% to act before emergency. Automate cleanup where safe.",
        "difficulty": "Medium",
        "time_estimate": 80
      },
      {
        "question": "You need to estimate a complex project with many unknowns. What approach should you take?",
        "options": [
          "Pick a random number",
          "Break into smaller tasks, estimate ranges (best/likely/worst), identify risks and unknowns, add buffer",
          "Always say 1 week",
          "Let someone else decide"
        ],
        "correct_answer": 1,
        "explanation": "Estimation with uncertainty: (1) decompose into smaller, more estimable tasks, (2) use three-point estimates (optimistic/likely/pessimistic), (3) identify dependencies and risks, (4) aggregate with uncertainty (Monte Carlo), (5) communicate confidence levels. Unknowns require discovery time (spikes). Past data helps. Cone of uncertainty narrows over time. Better to give ranges with confidence than false precision. Re-estimate as you learn.",
        "difficulty": "Hard",
        "time_estimate": 95
      }
    ],
    "Critical Thinking": [
      {
        "question": "A colleague claims 'Our new ML model is 95% accurate, so it's ready for production.' What's the issue with this reasoning?",
        "options": [
          "95% is always good enough",
          "Accuracy alone is insufficient - need precision, recall, and understanding of class distribution",
          "The model should be 100% accurate",
          "Accuracy is irrelevant"
        ],
        "correct_answer": 1,
        "explanation": "Accuracy is misleading especially with imbalanced classes. For 95% negative class, always predicting negative gives 95% accuracy but is useless. Need: (1) precision/recall/F1, (2) confusion matrix, (3) per-class metrics, (4) business impact of errors (FP vs FN cost). Critical thinking: question single metrics, consider context. Production readiness requires: performance on relevant metrics, robustness, monitoring, rollback plan.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "Manager says: 'If we double the team size, we'll finish in half the time.' What's the flaw in this logic?",
        "options": [
          "This is always true",
          "Ignores communication overhead, ramp-up time, and task dependencies (Brooks's Law)",
          "Should triple the team instead",
          "Team size doesn't matter"
        ],
        "correct_answer": 1,
        "explanation": "Brooks's Law: 'Adding manpower to a late software project makes it later.' Reasons: (1) new members need training (reduces productivity temporarily), (2) communication overhead grows quadratically (n(n-1)/2 pairs), (3) some tasks aren't parallelizable (dependencies), (4) coordination costs increase. Doubling team rarely doubles speed, often reduces it. Critical thinking: recognize false linear assumptions, consider system dynamics and constraints.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "A blog post claims 'Technology X is always better than Technology Y.' What should you question?",
        "options": [
          "Nothing, accept it as fact",
          "The context, trade-offs, use cases, and evidence supporting the claim",
          "The author's name only",
          "Grammar only"
        ],
        "correct_answer": 1,
        "explanation": "Critical evaluation of technology claims: (1) what's the context/use case?, (2) what are the trade-offs?, (3) what evidence supports this? (benchmarks? production experience?), (4) who benefits from this claim? (vendor?), (5) are there counter-examples? No technology is universally superior - all have trade-offs. Question absolutes ('always', 'never', 'best'). Consider: performance, complexity, cost, team expertise, ecosystem maturity.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "Two metrics show contradictory trends: user signups are up 50%, but revenue is down 20%. What might explain this?",
        "options": [
          "Impossible, ignore one metric",
          "Quality of signups changed (lower-paying users), pricing changes, free tier growth, or conversion rate drop",
          "Metrics are broken",
          "Revenue doesn't matter"
        ],
        "correct_answer": 1,
        "explanation": "Contradictory metrics require investigation: (1) segment analysis (who are new signups?), (2) cohort analysis (do new users behave differently?), (3) pricing changes, (4) free vs paid ratio, (5) conversion rate trends. Possible causes: viral growth in low-value segment, competitors targeting high-value users, product changes affecting monetization. Critical thinking: don't cherry-pick metrics, investigate correlations, understand the full picture.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "A study shows developers using IDE X are 30% more productive. Can you conclude IDE X causes increased productivity?",
        "options": [
          "Yes, definitely",
          "No - correlation doesn't imply causation; could be selection bias or confounding factors",
          "Yes, if the study used statistics",
          "No studies are reliable"
        ],
        "correct_answer": 1,
        "explanation": "Correlation ≠ causation. Potential explanations: (1) selection bias (better developers choose IDE X), (2) experience (senior devs use X), (3) confounding factors (X users have better hardware), (4) reverse causation (productive devs can afford/learn X). Need: randomized controlled trials or careful statistical controls. Critical thinking: identify alternative explanations, recognize biases. Mere correlation can't establish causality without ruling out confounders.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "Someone argues: 'We should use microservices because Google uses them.' What's the logical flaw?",
        "options": [
          "No flaw, always copy Google",
          "Appeal to authority and ignoring context - Google's scale/needs differ from most organizations",
          "Google is wrong",
          "Microservices never work"
        ],
        "correct_answer": 1,
        "explanation": "Logical fallacy: appeal to authority + ignoring context. Google's challenges (billions of users, thousands of developers) differ from most companies. Microservices add complexity - worth it at scale, overkill for small teams. Critical thinking: (1) understand your context, (2) evaluate trade-offs for YOUR situation, (3) don't cargo cult practices from different contexts. Good: 'X works at Google because of Y, do we have Y?' Bad: 'X works at Google, so we should do X.'",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "You notice feature usage dropped after a redesign. Manager says 'Users will adapt, give it time.' How should you respond?",
        "options": [
          "Agree and wait indefinitely",
          "Gather user feedback, analyze drop patterns, A/B test if possible, set decision timeline",
          "Roll back immediately without data",
          "Ignore users completely"
        ],
        "correct_answer": 1,
        "explanation": "Test assumptions with data: (1) quantify the drop (how much? which segments?), (2) gather qualitative feedback (surveys, support tickets), (3) understand why (confusion? missing features? preference?), (4) A/B test (keep old version for comparison), (5) set decision criteria (if X after Y time, then rollback). Some adaptation is normal, but sustained drops signal problems. Critical thinking: balance patience with responsiveness, use data not opinions.",
        "difficulty": "Medium",
        "time_estimate": 90
      },
      {
        "question": "A vendor claims their tool will 'reduce bugs by 90%.' What questions should you ask?",
        "options": [
          "Accept the claim immediately",
          "Evidence source, methodology, context, definition of 'bugs', comparison baseline",
          "No questions needed",
          "Only ask about price"
        ],
        "correct_answer": 1,
        "explanation": "Evaluate extraordinary claims: (1) what's the evidence? (case studies? controlled experiments?), (2) what's the methodology? (how measured?), (3) what's the baseline? (90% vs what?), (4) what's the context? (worked for whom? what domain?), (5) how do they define 'bugs'? (severity?). Vendors incentivized to oversell. Look for: peer-reviewed studies, multiple independent sources, realistic claims. Critical thinking: extraordinary claims require extraordinary evidence.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "Root cause analysis reveals 'human error' as the cause of an outage. Is this a satisfactory conclusion?",
        "options": [
          "Yes, fire the person responsible",
          "No - should investigate systemic issues that enabled the error (poor tooling, unclear processes, missing safeguards)",
          "Yes, humans are always the problem",
          "Ignore the outage"
        ],
        "correct_answer": 1,
        "explanation": "'Human error' is rarely the root cause - it's a symptom of systemic issues. Dig deeper with Five Whys: Why did they make the error? (unclear procedure) Why unclear? (not documented) Why not documented? (no process for documentation) Why no process? Blameless post-mortems focus on: (1) what systems failed to prevent error?, (2) how can we make it impossible/harder to repeat?, (3) automation, safeguards, documentation. Critical thinking: systems thinking over individual blame.",
        "difficulty": "Hard",
        "time_estimate": 100
      },
      {
        "question": "A report shows your app is #1 on a 'Top 10 Apps' list. What should you verify before sharing this achievement?",
        "options": [
          "Share immediately without checking",
          "Who created the list, criteria used, sample size, potential bias or payment for inclusion",
          "The font used in the report",
          "Nothing, rankings are always objective"
        ],
        "correct_answer": 1,
        "explanation": "Verify credibility of accolades: (1) who created it? (reputable source or pay-to-play?), (2) methodology (what criteria? sample size?), (3) selection bias (how were nominees chosen?), (4) was it paid/sponsored?, (5) when published (current or outdated?). Many 'awards' are marketing schemes. Critical thinking: distinguish legitimate recognition from promotional content. Verify before amplifying claims.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "Data shows users from source A have 2x higher conversion than source B. Should you cut budget from B and invest in A?",
        "options": [
          "Yes, immediately",
          "Not necessarily - need to check: sample size, user quality vs quantity, long-term value, and whether A can scale",
          "No, never change budgets",
          "Flip a coin"
        ],
        "correct_answer": 1,
        "explanation": "Avoid hasty conclusions: (1) statistical significance (is sample size adequate?), (2) short vs long-term value (higher churn in A?), (3) scalability (can A handle more volume? diminishing returns?), (4) cost per conversion (A might be more expensive), (5) strategic value (B might target important segment). Critical thinking: consider full picture, long-term effects, constraints. Optimize holistically, not on single metric.",
        "difficulty": "Hard",
        "time_estimate": 90
      },
      {
        "question": "You read: 'Developers who use TypeScript make 40% fewer bugs.' What confounding factors might explain this?",
        "options": [
          "None, TypeScript directly causes fewer bugs",
          "Developer experience, project complexity, team practices, code review rigor, testing culture",
          "TypeScript is magic",
          "The study must be wrong"
        ],
        "correct_answer": 1,
        "explanation": "Confounding factors: (1) selection (who chooses TypeScript? experienced devs?), (2) project maturity (TS used in newer, better-designed projects?), (3) team practices (teams adopting TS might also do better testing), (4) complexity (TS projects might be different domains), (5) code review culture. To establish causality need: randomized assignment or statistical controls. Critical thinking: identify lurking variables, alternative explanations.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "Proposal: 'Let's rewrite everything in Technology X because it's newer and better.' What's the problem with this reasoning?",
        "options": [
          "No problem, new is always better",
          "Assumes new = better, ignores rewrite costs/risks, and doesn't identify actual problems being solved",
          "Old technology is always better",
          "Technology doesn't matter"
        ],
        "correct_answer": 1,
        "explanation": "Question the premise: (1) what problems does current system have?, (2) will rewrite solve them or introduce new problems?, (3) what's the cost? (time, risk, opportunity cost), (4) can we incrementally improve instead?, (5) is team experienced in X? Rewrites often fail, take longer than estimated, introduce new bugs. New != better. Critical thinking: identify actual problems first, evaluate solutions against problems, consider costs/risks realistically. Strangler fig pattern > big rewrite.",
        "difficulty": "Hard",
        "time_estimate": 95
      },
      {
        "question": "Metric dashboard shows all green (targets met). Should you conclude everything is fine?",
        "options": [
          "Yes, celebrate and relax",
          "Not necessarily - check if metrics still align with goals, if gaming is happening, and if leading indicators show future problems",
          "No, always panic",
          "Ignore all metrics"
        ],
        "correct_answer": 1,
        "explanation": "Question metrics: (1) do they still measure what matters? (goals changed?), (2) Goodhart's Law: when measure becomes target, it ceases to be good measure (gaming?), (3) are they lagging indicators? (problem brewing not yet visible?), (4) are we missing important signals? Green doesn't mean perfect - might mean wrong metrics. Critical thinking: metrics are proxies not goals, can be gamed, need regular review. Look beyond dashboard to reality.",
        "difficulty": "Medium",
        "time_estimate": 85
      },
      {
        "question": "A popular influencer recommends an architecture pattern. Should you adopt it for your project?",
        "options": [
          "Yes, influencers are always right",
          "Evaluate based on your context, requirements, and trade-offs, not popularity",
          "No, never trust influencers",
          "Architecture doesn't matter"
        ],
        "correct_answer": 1,
        "explanation": "Evaluate independently: (1) what problem does it solve?, (2) do we have that problem?, (3) what are trade-offs?, (4) what's our context? (team size, scale, domain), (5) what's the evidence? (production use? only tutorials?). Influencers may lack your context, may oversimplify, or promote sponsors. Critical thinking: evaluate claims on merit, not popularity. Understand WHY before adopting WHAT. Context matters more than authority.",
        "difficulty": "Hard",
        "time_estimate": 90
      }
    ]
  },
  "user_progress": {
    "total_attempted": 228,
    "total_correct": 216,
    "category_stats": {
      "TensorFlow": {
        "attempted": 16,
        "correct": 16
      },
      "PyTorch": {
        "attempted": 15,
        "correct": 15
      },
      "REST APIs": {
        "attempted": 26,
        "correct": 26
      },
      "OOP": {
        "attempted": 21,
        "correct": 21
      },
      "Algorithms": {
        "attempted": 46,
        "correct": 43
      },
      "Problem Solving": {
        "attempted": 15,
        "correct": 15
      },
      "Critical Thinking": {
        "attempted": 5,
        "correct": 5
      },
      "Generative AI": {
        "attempted": 20,
        "correct": 20
      },
      "NLP": {
        "attempted": 21,
        "correct": 21
      },
      "Artificial Intelligence": {
        "attempted": 20,
        "correct": 16
      },
      "Deep Learning": {
        "attempted": 20,
        "correct": 15
      },
      "Pandas": {
        "attempted": 3,
        "correct": 3
      }
    },
    "bookmarked_questions": [
      {
        "category": "TensorFlow",
        "question_id": 0
      }
    ],
    "attempt_history": [
      {
        "timestamp": "2025-12-23T00:27:13.270379",
        "category": "TensorFlow",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:27:48.010873",
        "category": "TensorFlow",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:25:50.220280",
        "category": "TensorFlow",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:28:58.432401",
        "category": "TensorFlow",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:29:13.437886",
        "category": "TensorFlow",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:29:22.876995",
        "category": "TensorFlow",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:29:34.257755",
        "category": "TensorFlow",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:29:42.769161",
        "category": "TensorFlow",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:29:55.043446",
        "category": "TensorFlow",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:30:11.068542",
        "category": "TensorFlow",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:30:20.142714",
        "category": "TensorFlow",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:30:27.855713",
        "category": "TensorFlow",
        "question_id": 11,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:30:35.262678",
        "category": "TensorFlow",
        "question_id": 12,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:30:49.111940",
        "category": "TensorFlow",
        "question_id": 13,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:30:57.266644",
        "category": "TensorFlow",
        "question_id": 14,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:31:08.237562",
        "category": "PyTorch",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:31:16.331608",
        "category": "PyTorch",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:31:22.055303",
        "category": "PyTorch",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:31:26.941373",
        "category": "PyTorch",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:31:35.349168",
        "category": "PyTorch",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:31:43.452215",
        "category": "PyTorch",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:31:51.126954",
        "category": "PyTorch",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:34:47.387308",
        "category": "PyTorch",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:34:55.929769",
        "category": "PyTorch",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:32:36.391634",
        "category": "PyTorch",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:32:46.906978",
        "category": "PyTorch",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:32:54.652812",
        "category": "PyTorch",
        "question_id": 11,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:33:01.275335",
        "category": "PyTorch",
        "question_id": 12,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:33:08.029009",
        "category": "PyTorch",
        "question_id": 13,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:33:15.967911",
        "category": "PyTorch",
        "question_id": 14,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:33:40.576436",
        "category": "REST APIs",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:33:54.041976",
        "category": "REST APIs",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:34:00.242016",
        "category": "REST APIs",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:34:11.504411",
        "category": "REST APIs",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:34:19.680364",
        "category": "REST APIs",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:34:29.393115",
        "category": "REST APIs",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:34:40.953568",
        "category": "REST APIs",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:34:49.055768",
        "category": "REST APIs",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:35:00.968834",
        "category": "REST APIs",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:35:12.312132",
        "category": "REST APIs",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:35:25.108578",
        "category": "REST APIs",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:35:37.098547",
        "category": "REST APIs",
        "question_id": 11,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:35:51.030143",
        "category": "OOP",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:36:01.999181",
        "category": "OOP",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:36:08.604011",
        "category": "OOP",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:36:15.177597",
        "category": "OOP",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:36:23.065603",
        "category": "OOP",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:36:30.934861",
        "category": "OOP",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:36:47.493398",
        "category": "OOP",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:37:04.367380",
        "category": "OOP",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:37:13.284598",
        "category": "OOP",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:37:24.605865",
        "category": "OOP",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:37:35.882218",
        "category": "OOP",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:37:46.544158",
        "category": "OOP",
        "question_id": 11,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:37:55.948581",
        "category": "OOP",
        "question_id": 12,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:38:21.808351",
        "category": "OOP",
        "question_id": 13,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:38:33.686955",
        "category": "OOP",
        "question_id": 14,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:38:57.577077",
        "category": "Algorithms",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:39:10.731294",
        "category": "Algorithms",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:39:21.770538",
        "category": "Algorithms",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:39:31.564846",
        "category": "Algorithms",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:39:40.061079",
        "category": "Algorithms",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:39:55.275203",
        "category": "Algorithms",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:40:03.256971",
        "category": "Algorithms",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:40:09.354002",
        "category": "Algorithms",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:40:15.720928",
        "category": "Algorithms",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:40:26.791864",
        "category": "Algorithms",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:40:34.926826",
        "category": "Algorithms",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:40:54.414015",
        "category": "Algorithms",
        "question_id": 11,
        "correct": false
      },
      {
        "timestamp": "2025-12-23T00:41:33.903644",
        "category": "Algorithms",
        "question_id": 12,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:42:11.722747",
        "category": "Algorithms",
        "question_id": 13,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:42:42.244187",
        "category": "Algorithms",
        "question_id": 14,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:42:54.063322",
        "category": "Algorithms",
        "question_id": 15,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:43:23.719554",
        "category": "Algorithms",
        "question_id": 16,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:43:32.096275",
        "category": "Algorithms",
        "question_id": 17,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:43:43.649137",
        "category": "Algorithms",
        "question_id": 18,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:43:49.575065",
        "category": "Algorithms",
        "question_id": 19,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:47:22.596077",
        "category": "Problem Solving",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:47:35.876610",
        "category": "Problem Solving",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:47:59.987059",
        "category": "Problem Solving",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:48:55.588228",
        "category": "Problem Solving",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:49:12.483638",
        "category": "Problem Solving",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:50:09.117698",
        "category": "Problem Solving",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:50:19.317773",
        "category": "Problem Solving",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:50:32.012555",
        "category": "Problem Solving",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:50:37.692210",
        "category": "Problem Solving",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:51:05.051095",
        "category": "Problem Solving",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:51:13.499235",
        "category": "Problem Solving",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:51:20.799256",
        "category": "Problem Solving",
        "question_id": 11,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:51:31.889912",
        "category": "Problem Solving",
        "question_id": 12,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:51:52.453550",
        "category": "Problem Solving",
        "question_id": 13,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:51:57.382150",
        "category": "Problem Solving",
        "question_id": 14,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:53:29.912109",
        "category": "Critical Thinking",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:54:00.691533",
        "category": "Critical Thinking",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:54:22.448529",
        "category": "Critical Thinking",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:55:03.427685",
        "category": "Critical Thinking",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T00:55:10.658115",
        "category": "Critical Thinking",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:53:25.349373",
        "category": "Generative AI",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:51:39.585207",
        "category": "Generative AI",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:51:59.159445",
        "category": "Generative AI",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:52:34.272733",
        "category": "Generative AI",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:55:42.070733",
        "category": "Generative AI",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:56:33.392364",
        "category": "Generative AI",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:54:44.481505",
        "category": "Generative AI",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:55:46.249866",
        "category": "Generative AI",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:58:53.515298",
        "category": "Generative AI",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:59:17.950494",
        "category": "Generative AI",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:57:06.492721",
        "category": "Generative AI",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:01:52.400368",
        "category": "Generative AI",
        "question_id": 11,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:59:28.633963",
        "category": "Generative AI",
        "question_id": 12,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T18:59:55.598638",
        "category": "Generative AI",
        "question_id": 13,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:02:38.427957",
        "category": "Generative AI",
        "question_id": 14,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:02:44.560006",
        "category": "Generative AI",
        "question_id": 15,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:02:50.785540",
        "category": "Generative AI",
        "question_id": 16,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:03:00.802286",
        "category": "Generative AI",
        "question_id": 17,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:00:34.432174",
        "category": "Generative AI",
        "question_id": 18,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:00:48.341462",
        "category": "Generative AI",
        "question_id": 19,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:00:55.188174",
        "category": "NLP",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:03:39.423639",
        "category": "NLP",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:03:49.221365",
        "category": "NLP",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:04:03.535117",
        "category": "NLP",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:01:39.873351",
        "category": "NLP",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:04:55.883360",
        "category": "NLP",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:05:04.385273",
        "category": "NLP",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:02:43.046793",
        "category": "NLP",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:02:52.304976",
        "category": "NLP",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:07:02.386094",
        "category": "NLP",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:07:07.695353",
        "category": "NLP",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:07:16.851972",
        "category": "NLP",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:07:23.180676",
        "category": "NLP",
        "question_id": 11,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:08:01.756583",
        "category": "NLP",
        "question_id": 12,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:08:13.367031",
        "category": "NLP",
        "question_id": 13,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:08:26.350519",
        "category": "NLP",
        "question_id": 14,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:06:04.757580",
        "category": "NLP",
        "question_id": 15,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:10:06.502986",
        "category": "NLP",
        "question_id": 16,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:10:14.105041",
        "category": "NLP",
        "question_id": 17,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:10:24.161147",
        "category": "NLP",
        "question_id": 18,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:06:35.693881",
        "category": "NLP",
        "question_id": 19,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:09:23.476484",
        "category": "Artificial Intelligence",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:13:11.391588",
        "category": "Artificial Intelligence",
        "question_id": 1,
        "correct": false
      },
      {
        "timestamp": "2025-12-23T19:08:07.768639",
        "category": "Artificial Intelligence",
        "question_id": 2,
        "correct": false
      },
      {
        "timestamp": "2025-12-23T19:13:35.629351",
        "category": "Artificial Intelligence",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:11:15.475555",
        "category": "Artificial Intelligence",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:11:44.909181",
        "category": "Artificial Intelligence",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:11:52.360336",
        "category": "Artificial Intelligence",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:11:59.761110",
        "category": "Artificial Intelligence",
        "question_id": 7,
        "correct": false
      },
      {
        "timestamp": "2025-12-23T19:12:10.979851",
        "category": "Artificial Intelligence",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:12:25.209267",
        "category": "Artificial Intelligence",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:12:40.266032",
        "category": "Artificial Intelligence",
        "question_id": 10,
        "correct": false
      },
      {
        "timestamp": "2025-12-23T19:13:05.860453",
        "category": "Artificial Intelligence",
        "question_id": 11,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:13:13.558446",
        "category": "Artificial Intelligence",
        "question_id": 12,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:21:29.217102",
        "category": "Artificial Intelligence",
        "question_id": 13,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:21:37.237325",
        "category": "Artificial Intelligence",
        "question_id": 14,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:21:42.750827",
        "category": "Artificial Intelligence",
        "question_id": 15,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:21:47.389577",
        "category": "Artificial Intelligence",
        "question_id": 16,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:21:52.218107",
        "category": "Artificial Intelligence",
        "question_id": 17,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:22:12.968621",
        "category": "Artificial Intelligence",
        "question_id": 18,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:22:17.133810",
        "category": "Artificial Intelligence",
        "question_id": 19,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:22:29.243840",
        "category": "Deep Learning",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:22:37.409856",
        "category": "Deep Learning",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:22:48.922727",
        "category": "Deep Learning",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:22:57.392342",
        "category": "Deep Learning",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:25:52.424008",
        "category": "Deep Learning",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:25:59.940876",
        "category": "Deep Learning",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:26:05.432383",
        "category": "Deep Learning",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:23:40.099189",
        "category": "Deep Learning",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:47:11.036012",
        "category": "Deep Learning",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:44:40.424423",
        "category": "Deep Learning",
        "question_id": 9,
        "correct": false
      },
      {
        "timestamp": "2025-12-23T19:44:46.117394",
        "category": "Deep Learning",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:44:53.318418",
        "category": "Deep Learning",
        "question_id": 11,
        "correct": false
      },
      {
        "timestamp": "2025-12-23T19:45:01.011653",
        "category": "Deep Learning",
        "question_id": 12,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:45:06.014924",
        "category": "Deep Learning",
        "question_id": 13,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:45:11.647482",
        "category": "Deep Learning",
        "question_id": 14,
        "correct": false
      },
      {
        "timestamp": "2025-12-23T19:45:15.258078",
        "category": "Deep Learning",
        "question_id": 15,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:45:17.755944",
        "category": "Deep Learning",
        "question_id": 16,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:45:21.201775",
        "category": "Deep Learning",
        "question_id": 17,
        "correct": false
      },
      {
        "timestamp": "2025-12-23T19:45:28.648359",
        "category": "Deep Learning",
        "question_id": 18,
        "correct": true
      },
      {
        "timestamp": "2025-12-23T19:45:35.749749",
        "category": "Deep Learning",
        "question_id": 19,
        "correct": false
      },
      {
        "timestamp": "2025-12-24T16:43:47.834477",
        "category": "Pandas",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-24T16:44:00.181098",
        "category": "Pandas",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-24T16:44:16.351157",
        "category": "Pandas",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-24T16:45:04.912698",
        "category": "TensorFlow",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-24T16:46:14.325780",
        "category": "Algorithms",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-24T16:46:43.534462",
        "category": "Algorithms",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-24T16:46:56.868748",
        "category": "Algorithms",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-24T16:47:17.580459",
        "category": "Algorithms",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-24T16:48:46.010281",
        "category": "Algorithms",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-24T16:49:13.302854",
        "category": "Algorithms",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T09:40:59.375206",
        "category": "REST APIs",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T09:44:23.162472",
        "category": "REST APIs",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:15:53.828929",
        "category": "REST APIs",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:18:08.834794",
        "category": "REST APIs",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:20:38.668923",
        "category": "REST APIs",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:25:34.834332",
        "category": "REST APIs",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:28:14.044651",
        "category": "REST APIs",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:29:51.943735",
        "category": "REST APIs",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:30:47.561914",
        "category": "REST APIs",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:31:29.550986",
        "category": "REST APIs",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:32:18.360172",
        "category": "REST APIs",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:33:00.716183",
        "category": "REST APIs",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:33:36.635944",
        "category": "REST APIs",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:37:28.387639",
        "category": "REST APIs",
        "question_id": 11,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:39:10.978125",
        "category": "Algorithms",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:42:08.339560",
        "category": "Algorithms",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:45:28.306735",
        "category": "Algorithms",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:47:06.940354",
        "category": "Algorithms",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:48:19.768868",
        "category": "Algorithms",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:49:53.914551",
        "category": "Algorithms",
        "question_id": 5,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:51:23.094287",
        "category": "Algorithms",
        "question_id": 6,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T10:53:51.675922",
        "category": "Algorithms",
        "question_id": 7,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:05:33.533551",
        "category": "Algorithms",
        "question_id": 8,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:11:06.890339",
        "category": "Algorithms",
        "question_id": 9,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:11:34.336304",
        "category": "Algorithms",
        "question_id": 10,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:12:26.633362",
        "category": "Algorithms",
        "question_id": 11,
        "correct": false
      },
      {
        "timestamp": "2025-12-25T11:14:03.217619",
        "category": "Algorithms",
        "question_id": 12,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:14:35.941563",
        "category": "Algorithms",
        "question_id": 13,
        "correct": false
      },
      {
        "timestamp": "2025-12-25T11:17:12.103203",
        "category": "Algorithms",
        "question_id": 14,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:19:22.616786",
        "category": "Algorithms",
        "question_id": 15,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:19:46.923362",
        "category": "Algorithms",
        "question_id": 16,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:20:15.481668",
        "category": "Algorithms",
        "question_id": 17,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:21:27.577624",
        "category": "Algorithms",
        "question_id": 18,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:21:43.780884",
        "category": "Algorithms",
        "question_id": 19,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:22:30.099020",
        "category": "OOP",
        "question_id": 0,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:23:02.049049",
        "category": "OOP",
        "question_id": 1,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:23:36.863604",
        "category": "OOP",
        "question_id": 2,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:25:46.235257",
        "category": "OOP",
        "question_id": 3,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:26:47.121132",
        "category": "OOP",
        "question_id": 4,
        "correct": true
      },
      {
        "timestamp": "2025-12-25T11:27:39.857662",
        "category": "OOP",
        "question_id": 5,
        "correct": true
      }
    ]
  }
}